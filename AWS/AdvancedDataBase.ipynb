{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Περιεχόμενα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Files to be used\n",
    "\n",
    "# Paths for csv\n",
    "fcrime = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "fstations = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "fincome = 's3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv'\n",
    "fcodes = 's3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv'\n",
    "\n",
    "# Paths for .parquet\n",
    "fcrime_parq = 's3://groups-bucket-dblab-905418150721/group10/CrimeData.parquet'\n",
    "fstations_parq = 's3://groups-bucket-dblab-905418150721/group10/PoliceStations.parquet'\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = 's3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson'\n",
    "fgeofields = 's3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks_fields.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", DateType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Να υλοποιηθεί το __Query 1__ χρησιμοποιώντας τα DataFrame και RDD APIs. Να εκτελέσετε και τις δύο υλοποιήσεις με 4 Spark executors. Υπάρχει διαφορά στην επίδοση μεταξύ των δύο APIs; Αιτιολογήσετε την απάντησή σας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize into age groups\n",
    "categorized_df = crimes_df.withColumn('age_group',\n",
    "                    when(col('Vict Age').cast('int') < 18, 'Children')\n",
    "                    .when(\n",
    "                        ((col('Vict Age').cast('int') >= 18) & (col('Vict Age').cast('int') <= 24)), 'Young Adults'\n",
    "                        )\n",
    "                    .when(\n",
    "                        ((col('Vict Age').cast('int') >= 25) & (col('Vict Age').cast('int') <= 64)), 'Adults'\n",
    "                        )\n",
    "                    .when(col('Vict Age').cast('int') > 64, 'Elderly')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'AGGRAVATED ASSAULT'\n",
    "assault_df = categorized_df.filter(\n",
    "    col('Crm Cd Desc').contains('AGGRAVATED ASSAULT')\n",
    "    ) \\\n",
    "    .groupby('age_group') \\\n",
    "    .agg(count('*').alias('victim_count')) \\\n",
    "    .orderBy(col('victim_count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "assault_df.show()\n",
    "\n",
    "time_end = time.time()\n",
    "time_df = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δυστυχώς στα δεδομένα μας έχουμε μερικές περιπτώσεις όπου υπάρχει υποδιαστολή εντός quote marks (\"\"), δημιουργώντας έτσι θέμα στο parse.\n",
    "Για αυτό θα χρησιμοποιήσουμε μια βιβλιοθήκη της Python για την ανάγνωση των csv αρχείων και θα την περάσουμε μέσω mapping σε κάθε δεδομένο:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_line(line):\n",
    "    reader = csv.reader([line])\n",
    "    fields = next(reader)\n",
    "    return {\n",
    "        'DR_NO': fields[0],\n",
    "        'DateRptd': fields[1],\n",
    "        'DATEOCC': fields[2],\n",
    "        'TIMEOCC': fields[3],\n",
    "        'AREA': fields[4],\n",
    "        'AREANAME': fields[5],\n",
    "        'RptDistNo': fields[6],\n",
    "        'Part': fields[7],\n",
    "        'CrmCd': fields[8],\n",
    "        'CrmCdDesc': fields[9],\n",
    "        'Mocodes': fields[10],\n",
    "        'VictAge': int(fields[11]),\n",
    "        'VictSex': fields[12],\n",
    "        'VictDescent': fields[13],\n",
    "        'PremisCd': fields[14],\n",
    "        'PremisDesc': fields[15],\n",
    "        'WeaponUsedCd': fields[16],\n",
    "        'WeaponDesc': fields[17],\n",
    "        'Status': fields[18],\n",
    "        'CrmCd1': fields[19],\n",
    "        'CrmCd2': fields[20],\n",
    "        'CrmCd3': fields[21],\n",
    "        'CrmCd4': fields[22],\n",
    "        'LOCATION': fields[23],\n",
    "        'CrossStreet': fields[24],\n",
    "        'LAT': fields[25],\n",
    "        'LON': fields[26]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv as Text\n",
    "rdd = spark.sparkContext.textFile(fcrime)\n",
    "\n",
    "# Remove the header\n",
    "header = rdd.first()\n",
    "crimes_rdd = rdd.filter(lambda line: line != header).map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function to categorize age groups\n",
    "def categorize_age(crime):\n",
    "    age = crime['VictAge']\n",
    "    if age < 18:\n",
    "        return 'Children'\n",
    "    elif age >= 18 and age <= 24:\n",
    "        return 'Young Adults'\n",
    "    elif age >= 25 and age <= 64:\n",
    "        return 'Adults'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "    \n",
    "categorized_rdd = crimes_rdd.filter(lambda x: 'AGGRAVATED ASSAULT' in x['CrmCdDesc']) \\\n",
    "                    .map(lambda x: (categorize_age(x), 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b) \\\n",
    "                    .sortBy(lambda x: -x[1])\n",
    "\n",
    "categorized_rdd.collect()\n",
    "time_end = time.time()\n",
    "time_rdd = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παρατηρήσεις:\n",
    "\n",
    "Η διαφορά στην απόδοση μεταξύ των 2 μεθόδων δεν είναι τόσο εμφανής όσο αναμέναμε.\n",
    "Τα DataFrames αξιοποιούν το optimization και προσφέρουν γενικά μεγαλύτερη ταχύτητα, αν και στη συγκεκριμένη περίπτωση η διαφορά είναι μικρή.\n",
    "Αντιθέτως, τα RDD προσφέρουν μεγαλύτερη ευελιξία, αφού είναι low level, στην επεξεργασία των δεδομένων. \n",
    "\n",
    "Πιθανόν το μέγεθος των δεδομένων να μην είναι αρκετά μεγάλο ώστε να αρχίσει να φαίνεται μια ουσιαστική διαφορά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Time taken for DataFrame API: {time_df}.\n",
    "Time taken for RDD API: {time_rdd}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το __Query 2__ χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    crimes_df.AREA.cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(crimes_df.DateRptd).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table rsults\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time = time_end - time_start\n",
    "print(f'Time taken since creation of DF spark session to completion: {df_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes Table\n",
    "\n",
    "crimes_schema_sql = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", StringType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.format('csv') \\\n",
    "    .options(header='true', dateFormat='MM/dd/yyyy hh:mm:ss a') \\\n",
    "    .schema(crimes_schema_sql) \\\n",
    "    .load(fcrime)\n",
    "\n",
    "crimes_df = crimes_df.withColumn(\"DateRptd\", to_timestamp(\"DateRptd\", \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "                     .withColumn(\"DATEOCC\", to_timestamp(\"DATEOCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations Table\n",
    "\n",
    "stations_schema_sql = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(stations_schema_sql) \\\n",
    "    .load(fstations)\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql = \"\"\"WITH YearlyPrecinctStats AS ( \n",
    "    SELECT \n",
    "        YEAR(c.DateRptd) AS year,\n",
    "        s.DIVISION AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS closed_case_rate\n",
    "    FROM crimes c\n",
    "    JOIN stations s\n",
    "        ON CAST(c.AREA as INTEGER) = s.FID\n",
    "    GROUP BY YEAR(c.DateRptd), s.DIVISION\n",
    "    ),\n",
    "    rankedPrecincts AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM YearlyPrecinctStats\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM RankedPrecincts\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY year, ranking;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__ \n",
    "\n",
    "YearlyPrecinctStats\n",
    "- We need to group our data by year and department\n",
    "- Keep a count of all cases and a count of closed cases\n",
    "- Create the rate as a percentage\n",
    "\n",
    "RankedPrecincts\n",
    "- From YearlyPrecinctStats keep: year, precinct and closed_case_rate\n",
    "- We will need to create the ranking based on the closed cases rate of each department\n",
    "- For each year assign a ranking (starting at 1) in descending order\n",
    "\n",
    "Notes:\n",
    "\n",
    "The symbol '#' is not supported in SQL so it was changed to ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query2_sql).show()\n",
    "time_end = time.time()\n",
    "sql_time = time_end - time_start\n",
    "print(f\"Time taken since creation of spark session to query completion: {sql_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''Time taken for DataFrame API: {df_time:.2f}\n",
    "Time taken for SQL API: {sql_time:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι οι χρόνοι εκτέλεσης δεν παρουσιάζουν μεγάλες αποκλίσεις.\n",
    "Παραδόξως, οι χρόνοι εκτέλεσης για το SQL API είναι συνήθως σταθεροί για όσες φορές τρέξουμε τον κώδικα, ενώ στο DataFrame API παρατηρούμε μεγάλη απόκλιση μεταξύ των τιμών για πολλαπλές εκτελέσεις του κώδικα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 Part 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime,header=True, inferSchema=True)\n",
    "crimes_df.write.mode('overwrite').parquet(fcrime_parq)\n",
    "\n",
    "stations_df = spark.read.csv(fstations,header=True, inferSchema=True)\n",
    "stations_df.write.mode('overwrite').parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιήσουμε το DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.parquet(fcrime_parq)\n",
    "stations_df = spark.read.parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = crimes_df.withColumn('Date Rptd', to_timestamp('Date Rptd', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    col('AREA ').cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(col('Date Rptd')).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table results\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time_parq = time_end - time_start\n",
    "print(f'Time taken for DF with parquet to completion: {df_time_parq:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας τα parquet δεδομένα, παρατηρούμε σταθερά μια αρκετά μεγάλη βελτίωση στην ταχύτητα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 3 χρησιμοποιώντας DataFrame ή SQL API. Χρησιμοποιήστε τις μεθόδους hint & explain για να βρείτε ποιες στρατηγικές join χρησιμοποιεί ο catalyst optimizer.\n",
    "Πειραματιστείτε αναγκάζοντας το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές (μεταξύ\n",
    "των BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL) και σχολιάστε τα αποτελέσματα\n",
    "που παρατηρείτε. Ποιά (ή ποιές) από τις διαθέσιμες στρατηγικές join του Spark είναι καταλληλότερη(ες) και γιατί;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_schema = StructType([\n",
    "    StructField('ZipCode', IntegerType()),\n",
    "    StructField('Community', StringType()),\n",
    "    StructField('Income', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = spark.read.csv(fincome, header=True, schema=income_schema)\n",
    "\n",
    "# Remove the $ character\n",
    "income_df = income_df.withColumn(\n",
    "    'Income', \n",
    "    regexp_replace(col('Income'), r\"[$,]\", \"\").cast('float')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_flattened_df = flattened_df.filter(col('CITY') == 'Los Angeles')\n",
    "crimes_df = crimes_df.filter(\n",
    "    (col('LAT') != 0) & (col('LON') != 0)\n",
    ")\n",
    "\n",
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "income_per_area_df = income_df.join(la_flattened_df, col('ZipCode') == la_flattened_df['ZCTA10'])\n",
    "\n",
    "# Table for Query 4\n",
    "q4 = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry'])\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('broadcast'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('broadcast'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_merge = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "result_df_merge = result_df_merge.join(\n",
    "    income_per_area_df.hint('merge'),\n",
    "    ST_Within(result_df_merge['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "#result_df_merge.show()\n",
    "\n",
    "result_df_merge.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_hash'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_hush'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "#result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_replicate_nl'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_replicate_nl'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "#result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 4 χρησιμοποιώντας το DataFrame ή SQL API. Να εκτελέσετε την υλοποίησή σας εφαρμόζοντας κλιμάκωση στο σύνολο των υπολογιστικών πόρων που θα χρησιμοποιήσετε: Συγκεκριμένα, καλείστε να εκτελέστε την υλοποίησή σας σε 2 executors με τα ακόλουθα configurations:\n",
    "\n",
    "- 1 core/2 GB memory\n",
    "- 2 cores/4GB memory\n",
    "- 4 cores/8GB memory\n",
    "\n",
    "Σχολιάστε τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 core / 2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 1\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 cores / 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 2\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 cores / 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 3\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
