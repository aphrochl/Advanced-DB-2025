{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Περιεχόμενα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to be used\n",
    "\n",
    "fcrime = \"Data\\Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "fstations = \"Data\\LA_Police_Stations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF/SQL API\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", DateType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    crimes_df.AREA.cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(crimes_df.DateRptd).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table rsults\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time = time_end - time_start\n",
    "print(f'Time taken since creation of DF spark session to completion: {df_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes Table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", StringType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.format('csv') \\\n",
    "    .options(header='true', dateFormat='MM/dd/yyyy hh:mm:ss a') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(fcrime)\n",
    "\n",
    "crimes_df = crimes_df.withColumn(\"DateRptd\", to_timestamp(\"DateRptd\", \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "                     .withColumn(\"DATEOCC\", to_timestamp(\"DATEOCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations Table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(stations_schema) \\\n",
    "    .load(fstations)\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql = \"\"\"WITH YearlyPrecinctStats AS ( \n",
    "    SELECT \n",
    "        YEAR(c.DateRptd) AS year,\n",
    "        s.DIVISION AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS closed_case_rate\n",
    "    FROM crimes c\n",
    "    JOIN stations s\n",
    "        ON CAST(c.AREA as INTEGER) = s.FID\n",
    "    GROUP BY YEAR(c.DateRptd), s.DIVISION\n",
    "    ),\n",
    "    rankedPrecincts AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM YearlyPrecinctStats\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM RankedPrecincts\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY year, ranking;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__ \n",
    "\n",
    "YearlyPrecinctStats\n",
    "- We need to group our data by year and department\n",
    "- Keep a count of all cases and a count of closed cases\n",
    "- Create the rate as a percentage\n",
    "\n",
    "RankedPrecincts\n",
    "- From YearlyPrecinctStats keep: year, precinct and closed_case_rate\n",
    "- We will need to create the ranking based on the closed cases rate of each department\n",
    "- For each year assign a ranking (starting at 1) in descending order\n",
    "\n",
    "Notes:\n",
    "\n",
    "The symbol '#' is not supported in SQL so it was changed to ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query2_sql).show()\n",
    "time_end = time.time()\n",
    "sql_time = time_end - time_start\n",
    "print(f\"Time taken since creation of spark session to query completion: {sql_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''Time taken for DataFrame API: {df_time:.2f}\n",
    "Time taken for SQL API: {sql_time:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ο χρόνος εκτέλεσης του DataFrame API είναι περίπου διπλάσιος από του SQL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime,header=True, inferSchema=True)\n",
    "crimes_df.write.parquet('Data/Crime.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
