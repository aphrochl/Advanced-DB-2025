{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Περιεχόμενα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to be used\n",
    "\n",
    "# Paths for csv\n",
    "fcrime = \"..\\Data\\Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "fstations = \"..\\Data\\LA_Police_Stations.csv\"\n",
    "fincome = \"..\\Data\\LA_income_2015.csv\"\n",
    "fcodes = \"..\\Data\\RE_codes.csv\"\n",
    "\n",
    "# Paths for .parquet\n",
    "fcrime_parq = \"..\\Data\\CrimeData.parquet\"\n",
    "fstations_parq = \"..\\Data\\PoliceStations.parquet\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"../Data/2010_Census_Blocks.geojson\"\n",
    "fgeofields = \"../Data/2010_Census_Blocks_fields.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Advanced DB\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", DateType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Να υλοποιηθεί το __Query 1__ χρησιμοποιώντας τα DataFrame και RDD APIs. Να εκτελέσετε και τις δύο υλοποιήσεις με 4 Spark executors. Υπάρχει διαφορά στην επίδοση μεταξύ των δύο APIs; Αιτιολογήσετε την απάντησή σας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize into age groups\n",
    "categorized_df = crimes_df.withColumn('age_group',\n",
    "                    when(col('Vict Age').cast('int') < 18, 'Children')\n",
    "                    .when(\n",
    "                        ((col('Vict Age').cast('int') >= 18) & (col('Vict Age').cast('int') <= 24)), 'Young Adults'\n",
    "                        )\n",
    "                    .when(\n",
    "                        ((col('Vict Age').cast('int') >= 25) & (col('Vict Age').cast('int') <= 64)), 'Adults'\n",
    "                        )\n",
    "                    .when(col('Vict Age').cast('int') > 64, 'Elderly')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'AGGRAVATED ASSAULT'\n",
    "assault_df = categorized_df.filter(\n",
    "    col('Crm Cd Desc').contains('AGGRAVATED ASSAULT')\n",
    "    ) \\\n",
    "    .groupby('age_group') \\\n",
    "    .agg(count('*').alias('victim_count')) \\\n",
    "    .orderBy(col('victim_count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|   age_group|victim_count|\n",
      "+------------+------------+\n",
      "|      Adults|       72610|\n",
      "|Young Adults|       23472|\n",
      "|    Children|       10724|\n",
      "|     Elderly|        3099|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "assault_df.show()\n",
    "\n",
    "time_end = time.time()\n",
    "time_df = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δυστυχώς στα δεδομένα μας έχουμε μερικές περιπτώσεις όπου υπάρχει υποδιαστολή εντός quote marks (\"\"), δημιουργώντας έτσι θέμα στο parse.\n",
    "Για αυτό θα χρησιμοποιήσουμε μια βιβλιοθήκη της Python για την ανάγνωση των csv αρχείων και θα την περάσουμε μέσω mapping σε κάθε δεδομένο:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_line(line):\n",
    "    reader = csv.reader([line])\n",
    "    fields = next(reader)\n",
    "    return {\n",
    "        'DR_NO': fields[0],\n",
    "        'DateRptd': fields[1],\n",
    "        'DATEOCC': fields[2],\n",
    "        'TIMEOCC': fields[3],\n",
    "        'AREA': fields[4],\n",
    "        'AREANAME': fields[5],\n",
    "        'RptDistNo': fields[6],\n",
    "        'Part': fields[7],\n",
    "        'CrmCd': fields[8],\n",
    "        'CrmCdDesc': fields[9],\n",
    "        'Mocodes': fields[10],\n",
    "        'VictAge': int(fields[11]),\n",
    "        'VictSex': fields[12],\n",
    "        'VictDescent': fields[13],\n",
    "        'PremisCd': fields[14],\n",
    "        'PremisDesc': fields[15],\n",
    "        'WeaponUsedCd': fields[16],\n",
    "        'WeaponDesc': fields[17],\n",
    "        'Status': fields[18],\n",
    "        'CrmCd1': fields[19],\n",
    "        'CrmCd2': fields[20],\n",
    "        'CrmCd3': fields[21],\n",
    "        'CrmCd4': fields[22],\n",
    "        'LOCATION': fields[23],\n",
    "        'CrossStreet': fields[24],\n",
    "        'LAT': fields[25],\n",
    "        'LON': fields[26]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv as Text\n",
    "rdd = spark.sparkContext.textFile(fcrime)\n",
    "\n",
    "# Remove the header\n",
    "header = rdd.first()\n",
    "crimes_rdd = rdd.filter(lambda line: line != header).map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function to categorize age groups\n",
    "def categorize_age(crime):\n",
    "    age = crime['VictAge']\n",
    "    if age < 18:\n",
    "        return 'Children'\n",
    "    elif age >= 18 and age <= 24:\n",
    "        return 'Young Adults'\n",
    "    elif age >= 25 and age <= 64:\n",
    "        return 'Adults'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "    \n",
    "categorized_rdd = crimes_rdd.filter(lambda x: 'AGGRAVATED ASSAULT' in x['CrmCdDesc']) \\\n",
    "                    .map(lambda x: (categorize_age(x), 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b) \\\n",
    "                    .sortBy(lambda x: -x[1])\n",
    "\n",
    "categorized_rdd.collect()\n",
    "time_end = time.time()\n",
    "time_rdd = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παρατηρήσεις:\n",
    "\n",
    "Η διαφορά στην απόδοση μεταξύ των 2 μεθόδων δεν είναι τόσο εμφανής όσο αναμέναμε.\n",
    "Τα DataFrames αξιοποιούν το optimization και προσφέρουν γενικά μεγαλύτερη ταχύτητα, αν και στη συγκεκριμένη περίπτωση η διαφορά είναι μικρή.\n",
    "Αντιθέτως, τα RDD προσφέρουν μεγαλύτερη ευελιξία, αφού είναι low level, στην επεξεργασία των δεδομένων. \n",
    "\n",
    "Πιθανόν το μέγεθος των δεδομένων να μην είναι αρκετά μεγάλο ώστε να αρχίσει να φαίνεται μια ουσιαστική διαφορά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for DataFrame API: 4.23017692565918.\n",
      "Time taken for RDD API: 75.28375029563904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Time taken for DataFrame API: {time_df}.\n",
    "Time taken for RDD API: {time_rdd}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το __Query 2__ χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    crimes_df.AREA.cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(crimes_df.DateRptd).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table rsults\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of DF spark session to completion: 4.97 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time = time_end - time_start\n",
    "print(f'Time taken since creation of DF spark session to completion: {df_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes Table\n",
    "\n",
    "crimes_schema_sql = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", StringType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.format('csv') \\\n",
    "    .options(header='true', dateFormat='MM/dd/yyyy hh:mm:ss a') \\\n",
    "    .schema(crimes_schema_sql) \\\n",
    "    .load(fcrime)\n",
    "\n",
    "crimes_df = crimes_df.withColumn(\"DateRptd\", to_timestamp(\"DateRptd\", \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "                     .withColumn(\"DATEOCC\", to_timestamp(\"DATEOCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations Table\n",
    "\n",
    "stations_schema_sql = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(stations_schema_sql) \\\n",
    "    .load(fstations)\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql = \"\"\"WITH YearlyPrecinctStats AS ( \n",
    "    SELECT \n",
    "        YEAR(c.DateRptd) AS year,\n",
    "        s.DIVISION AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS closed_case_rate\n",
    "    FROM crimes c\n",
    "    JOIN stations s\n",
    "        ON CAST(c.AREA as INTEGER) = s.FID\n",
    "    GROUP BY YEAR(c.DateRptd), s.DIVISION\n",
    "    ),\n",
    "    rankedPrecincts AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM YearlyPrecinctStats\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM RankedPrecincts\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY year, ranking;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__ \n",
    "\n",
    "YearlyPrecinctStats\n",
    "- We need to group our data by year and department\n",
    "- Keep a count of all cases and a count of closed cases\n",
    "- Create the rate as a percentage\n",
    "\n",
    "RankedPrecincts\n",
    "- From YearlyPrecinctStats keep: year, precinct and closed_case_rate\n",
    "- We will need to create the ranking based on the closed cases rate of each department\n",
    "- For each year assign a ranking (starting at 1) in descending order\n",
    "\n",
    "Notes:\n",
    "\n",
    "The symbol '#' is not supported in SQL so it was changed to ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+\n",
      "|year|  precinct| closed_case_rate|ranking|\n",
      "+----+----------+-----------------+-------+\n",
      "|2010| SOUTHEAST|32.94735585531813|      1|\n",
      "|2010|DEVONSHIRE|31.96270619172842|      2|\n",
      "|2010| SOUTHWEST|29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.21216768916155|      1|\n",
      "|2011| SOUTHEAST|32.51177963030083|      2|\n",
      "|2011| SOUTHWEST|28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.41481831052383|      1|\n",
      "|2012| SOUTHEAST|32.94641810294290|      2|\n",
      "|2012| SOUTHWEST|29.81513327601032|      3|\n",
      "|2013|DEVONSHIRE|33.52812271731191|      1|\n",
      "|2013| SOUTHEAST|32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.16422459266206|      3|\n",
      "|2014|HOLLENBECK|31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.31198995605775|      2|\n",
      "|2014|  FOOTHILL|31.16279069767442|      3|\n",
      "|2015|HOLLENBECK|32.64134698172773|      1|\n",
      "|2015|  WILSHIRE|30.27597402597403|      2|\n",
      "|2015|  FOOTHILL|30.17946067838016|      3|\n",
      "|2016|HOLLENBECK|31.88075572011773|      1|\n",
      "|2016|  WILSHIRE|31.54798761609907|      2|\n",
      "+----+----------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of spark session to query completion: 4.83 seconds\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query2_sql).show()\n",
    "time_end = time.time()\n",
    "sql_time = time_end - time_start\n",
    "print(f\"Time taken since creation of spark session to query completion: {sql_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for DataFrame API: 4.97\n",
      "Time taken for SQL API: 4.83\n"
     ]
    }
   ],
   "source": [
    "print(f'''Time taken for DataFrame API: {df_time:.2f}\n",
    "Time taken for SQL API: {sql_time:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι οι χρόνοι εκτέλεσης δεν παρουσιάζουν μεγάλες αποκλίσεις.\n",
    "Παραδόξως, οι χρόνοι εκτέλεσης για το SQL API είναι συνήθως σταθεροί για όσες φορές τρέξουμε τον κώδικα, ενώ στο DataFrame API παρατηρούμε μεγάλη απόκλιση μεταξύ των τιμών για πολλαπλές εκτελέσεις του κώδικα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .config(\"spark.hadoop.fs.hdfs.impl.disable.cache\", \"true\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\") \\\n",
    "    .appName(\"Query 2 Part 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime,header=True, inferSchema=True)\n",
    "crimes_df.write.mode('overwrite').parquet(fcrime_parq)\n",
    "\n",
    "stations_df = spark.read.csv(fstations,header=True, inferSchema=True)\n",
    "stations_df.write.mode('overwrite').parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιήσουμε το DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.parquet(fcrime_parq)\n",
    "stations_df = spark.read.parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = crimes_df.withColumn('Date Rptd', to_timestamp('Date Rptd', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    col('AREA ').cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(col('Date Rptd')).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table results\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for DF with parquet to completion: 3.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time_parq = time_end - time_start\n",
    "print(f'Time taken for DF with parquet to completion: {df_time_parq:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας τα parquet δεδομένα, παρατηρούμε σταθερά μια αρκετά μεγάλη βελτίωση στην ταχύτητα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 3 χρησιμοποιώντας DataFrame ή SQL API. Χρησιμοποιήστε τις μεθόδους hint & explain για να βρείτε ποιες στρατηγικές join χρησιμοποιεί ο catalyst optimizer.\n",
    "Πειραματιστείτε αναγκάζοντας το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές (μεταξύ\n",
    "των BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL) και σχολιάστε τα αποτελέσματα\n",
    "που παρατηρείτε. Ποιά (ή ποιές) από τις διαθέσιμες στρατηγικές join του Spark είναι καταλληλότερη(ες) και γιατί;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_schema = StructType([\n",
    "    StructField('ZipCode', IntegerType()),\n",
    "    StructField('Community', StringType()),\n",
    "    StructField('Income', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = spark.read.csv(fincome, header=True, schema=income_schema)\n",
    "\n",
    "# Remove the $ character\n",
    "income_df = income_df.withColumn(\n",
    "    'Income', \n",
    "    regexp_replace(col('Income'), r\"[$,]\", \"\").cast('float')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_flattened_df = flattened_df.filter(col('CITY') == 'Los Angeles')\n",
    "crimes_df = crimes_df.filter(\n",
    "    (col('LAT') != 0) & (col('LON') != 0)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [ZipCode#1159], [cast(ZCTA10#1055 as int)], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1127]\n",
      "   :  +- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "   :     +- Filter isnotnull(ZipCode#1159)\n",
      "   :        +- FileScan csv [ZipCode#1159,Community#1160,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Community:string,Income:string>\n",
      "   +- Project [features#1022.properties.BG10 AS BG10#1031, features#1022.properties.BG10FIP10 AS BG10FIP10#1032, features#1022.properties.BG12 AS BG12#1033, features#1022.properties.CB10 AS CB10#1034, features#1022.properties.CEN_FIP13 AS CEN_FIP13#1035, features#1022.properties.CITY AS CITY#1036, features#1022.properties.CITYCOM AS CITYCOM#1037, features#1022.properties.COMM AS COMM#1038, features#1022.properties.CT10 AS CT10#1039, features#1022.properties.CT12 AS CT12#1040, features#1022.properties.CTCB10 AS CTCB10#1041, features#1022.properties.HD_2012 AS HD_2012#1042L, features#1022.properties.HD_NAME AS HD_NAME#1043, features#1022.properties.HOUSING10 AS HOUSING10#1044L, features#1022.properties.LA_FIP10 AS LA_FIP10#1045, features#1022.properties.OBJECTID AS OBJECTID#1046L, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.PUMA10 AS PUMA10#1048, features#1022.properties.SPA_2012 AS SPA_2012#1049L, features#1022.properties.SPA_NAME AS SPA_NAME#1050, features#1022.properties.SUP_DIST AS SUP_DIST#1051, features#1022.properties.SUP_LABEL AS SUP_LABEL#1052, features#1022.properties.ShapeSTArea AS ShapeSTArea#1053, features#1022.properties.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "      +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND isnotnull(features#1022.properties.ZCTA10))\n",
      "         +- Generate explode(features#1014), false, [features#1022]\n",
      "            +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "               +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "income_per_area_df = income_df.join(la_flattened_df, col('ZipCode') == la_flattened_df['ZCTA10'])\n",
    "\n",
    "income_per_area_df.explain()\n",
    "\n",
    "# Table for Query 4\n",
    "q4 = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry'])\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake|46.097391668899746|0.001780055400923...|\n",
      "|     Panorama City|59.679510369299315|0.001597238227038...|\n",
      "|     Baldwin Hills| 78.72397418970363|0.002119263022880266|\n",
      "| Little Bangladesh| 79.19429343714515|0.002037585700718...|\n",
      "|   Wilshire Center| 79.91559362366671|0.002108441605087...|\n",
      "|   University Park| 84.82775008029655|0.003672160928616...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.19512628794847|0.004395147375951249|\n",
      "|     Vermont Vista| 95.96195522120188|0.003189405043223...|\n",
      "|        South Park| 96.07853726620282|0.003380633455086159|\n",
      "|        Pico-Union|100.70963557402804|0.003261075176157...|\n",
      "|    Toluca Terrace|102.38366107576634|0.002111046847888953|\n",
      "|        West Adams|104.16825869722096|0.003524301019893139|\n",
      "|          Van Nuys|104.96980872032377|0.002409082925590...|\n",
      "|    Vernon Central|108.06714827739671|0.003725417785654...|\n",
      "|         Koreatown| 111.3956805264591| 0.00320934587109571|\n",
      "|           Central|111.73594532501643|0.003708236722048...|\n",
      "|    East Hollywood|112.66524150058802|0.002808127994863853|\n",
      "|       North Hills|116.05924532549375|0.002047854352339578|\n",
      "|   Harvard Heights|116.34242568549196|0.003234721578082767|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#1619 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#1619 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1464]\n",
      "      +- HashAggregate(keys=[COMM#1038], functions=[sum(Income#1165), sum(POP_2010#1047L), count(1)])\n",
      "         +- Exchange hashpartitioning(COMM#1038, 200), ENSURE_REQUIREMENTS, [plan_id=1461]\n",
      "            +- HashAggregate(keys=[COMM#1038], functions=[partial_sum(Income#1165), partial_sum(POP_2010#1047L), partial_count(1)])\n",
      "               +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "                  +- RangeJoin geom#1239: geometry, geometry#1025: geometry, WITHIN\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1239]\n",
      "                     :  +- Filter ((((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND NOT (LAT#1196 = 0.0)) AND NOT (LON#1197 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan csv [LAT#1196,LON#1197] Batched: false, DataFilters: [isnotnull(LAT#1196), isnotnull(LON#1197), NOT (LAT#1196 = 0.0), NOT (LON#1197 = 0.0), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:float,LON:float>\n",
      "                     +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "                        +- BroadcastHashJoin [ZipCode#1159], [cast(ZCTA10#1055 as int)], Inner, BuildLeft, false\n",
      "                           :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1454]\n",
      "                           :  +- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "                           :     +- Filter isnotnull(ZipCode#1159)\n",
      "                           :        +- FileScan csv [ZipCode#1159,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                           +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                              +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                                 +- Generate explode(features#1014), false, [features#1022]\n",
      "                                    +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                                       +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()\n",
    "\n",
    "result_df.explain()\n",
    "\n",
    "time_end = time.time()\n",
    "timeQ3Base = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['IncomePerPerson ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#1920, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#1923]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, IncomePerPerson: double, CrimesPerPerson: double\n",
      "Sort [IncomePerPerson#1920 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#1920, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#1923]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [IncomePerPerson#1920 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#1920, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#1923]\n",
      "   +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=broadcast)\n",
      "         :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1656]\n",
      "         :  +- Filter (((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND (NOT (LAT#1196 = 0.0) AND NOT (LON#1197 = 0.0))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "         +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "            +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int)), rightHint=(strategy=broadcast)\n",
      "               :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "               :  +- Filter isnotnull(ZipCode#1159)\n",
      "               :     +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "               +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                  +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                     +- Generate explode(features#1014), [0], false, [features#1022]\n",
      "                        +- Project [features#1014]\n",
      "                           +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#1920 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#1920 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1554]\n",
      "      +- HashAggregate(keys=[COMM#1038], functions=[sum(Income#1165), sum(POP_2010#1047L), count(1)], output=[COMM#1038, IncomePerPerson#1920, CrimesPerPerson#1923])\n",
      "         +- Exchange hashpartitioning(COMM#1038, 200), ENSURE_REQUIREMENTS, [plan_id=1551]\n",
      "            +- HashAggregate(keys=[COMM#1038], functions=[partial_sum(Income#1165), partial_sum(POP_2010#1047L), partial_count(1)], output=[COMM#1038, sum#1930, sum#1931L, count#1932L])\n",
      "               +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "                  +- BroadcastIndexJoin geom#1656: geometry, RightSide, LeftSide, Inner, WITHIN ST_WITHIN(geom#1656, geometry#1025)\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1656]\n",
      "                     :  +- Filter ((((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND NOT (LAT#1196 = 0.0)) AND NOT (LON#1197 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan csv [LAT#1196,LON#1197] Batched: false, DataFilters: [isnotnull(LAT#1196), isnotnull(LON#1197), NOT (LAT#1196 = 0.0), NOT (LON#1197 = 0.0), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:float,LON:float>\n",
      "                     +- SpatialIndex geometry#1025: geometry, RTREE, false, false\n",
      "                        +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "                           +- BroadcastHashJoin [ZipCode#1159], [cast(ZCTA10#1055 as int)], Inner, BuildRight, false\n",
      "                              :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "                              :  +- Filter isnotnull(ZipCode#1159)\n",
      "                              :     +- FileScan csv [ZipCode#1159,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[2, string, true] as int) as bigint)),false), [plan_id=1543]\n",
      "                                 +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                                    +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                                       +- Generate explode(features#1014), false, [features#1022]\n",
      "                                          +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                                             +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake|46.097391668899746|0.001780055400923...|\n",
      "|     Panorama City|59.679510369299315|0.001597238227038...|\n",
      "|     Baldwin Hills| 78.72397418970363|0.002119263022880266|\n",
      "| Little Bangladesh| 79.19429343714515|0.002037585700718...|\n",
      "|   Wilshire Center| 79.91559362366671|0.002108441605087...|\n",
      "|   University Park| 84.82775008029655|0.003672160928616...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.19512628794847|0.004395147375951249|\n",
      "|     Vermont Vista| 95.96195522120188|0.003189405043223...|\n",
      "|        South Park| 96.07853726620282|0.003380633455086159|\n",
      "|        Pico-Union|100.70963557402804|0.003261075176157...|\n",
      "|    Toluca Terrace|102.38366107576634|0.002111046847888953|\n",
      "|        West Adams|104.16825869722096|0.003524301019893139|\n",
      "|          Van Nuys|104.96980872032377|0.002409082925590...|\n",
      "|    Vernon Central|108.06714827739671|0.003725417785654...|\n",
      "|         Koreatown| 111.3956805264591| 0.00320934587109571|\n",
      "|           Central|111.73594532501643|0.003708236722048...|\n",
      "|    East Hollywood|112.66524150058802|0.002808127994863853|\n",
      "|       North Hills|116.05924532549375|0.002047854352339578|\n",
      "|   Harvard Heights|116.34242568549196|0.003234721578082767|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('broadcast'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('broadcast'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ3Broadcast = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake|46.097391668899746|0.001780055400923...|\n",
      "|     Panorama City|59.679510369299315|0.001597238227038...|\n",
      "|     Baldwin Hills| 78.72397418970363|0.002119263022880266|\n",
      "| Little Bangladesh| 79.19429343714515|0.002037585700718...|\n",
      "|   Wilshire Center| 79.91559362366671|0.002108441605087...|\n",
      "|   University Park| 84.82775008029655|0.003672160928616...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.19512628794847|0.004395147375951249|\n",
      "|     Vermont Vista| 95.96195522120188|0.003189405043223...|\n",
      "|        South Park| 96.07853726620282|0.003380633455086159|\n",
      "|        Pico-Union|100.70963557402804|0.003261075176157...|\n",
      "|    Toluca Terrace|102.38366107576634|0.002111046847888953|\n",
      "|        West Adams|104.16825869722096|0.003524301019893139|\n",
      "|          Van Nuys|104.96980872032377|0.002409082925590...|\n",
      "|    Vernon Central|108.06714827739671|0.003725417785654...|\n",
      "|         Koreatown| 111.3956805264591| 0.00320934587109571|\n",
      "|           Central|111.73594532501643|0.003708236722048...|\n",
      "|    East Hollywood|112.66524150058802|0.002808127994863853|\n",
      "|       North Hills|116.05924532549375|0.002047854352339578|\n",
      "|   Harvard Heights|116.34242568549196|0.003234721578082767|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['IncomePerPerson ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2163, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2166]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=merge)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, IncomePerPerson: double, CrimesPerPerson: double\n",
      "Sort [IncomePerPerson#2163 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2163, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2166]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=merge)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [IncomePerPerson#2163 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2163, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2166]\n",
      "   +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=merge)\n",
      "         :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1957]\n",
      "         :  +- Filter (((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND (NOT (LAT#1196 = 0.0) AND NOT (LON#1197 = 0.0))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "         +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "            +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int)), rightHint=(strategy=broadcast)\n",
      "               :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "               :  +- Filter isnotnull(ZipCode#1159)\n",
      "               :     +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "               +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                  +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                     +- Generate explode(features#1014), [0], false, [features#1022]\n",
      "                        +- Project [features#1014]\n",
      "                           +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#2163 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#2163 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2120]\n",
      "      +- HashAggregate(keys=[COMM#1038], functions=[sum(Income#1165), sum(POP_2010#1047L), count(1)], output=[COMM#1038, IncomePerPerson#2163, CrimesPerPerson#2166])\n",
      "         +- Exchange hashpartitioning(COMM#1038, 200), ENSURE_REQUIREMENTS, [plan_id=2117]\n",
      "            +- HashAggregate(keys=[COMM#1038], functions=[partial_sum(Income#1165), partial_sum(POP_2010#1047L), partial_count(1)], output=[COMM#1038, sum#2182, sum#2183L, count#2184L])\n",
      "               +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "                  +- RangeJoin geom#1957: geometry, geometry#1025: geometry, WITHIN\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1957]\n",
      "                     :  +- Filter ((((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND NOT (LAT#1196 = 0.0)) AND NOT (LON#1197 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan csv [LAT#1196,LON#1197] Batched: false, DataFilters: [isnotnull(LAT#1196), isnotnull(LON#1197), NOT (LAT#1196 = 0.0), NOT (LON#1197 = 0.0), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:float,LON:float>\n",
      "                     +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "                        +- BroadcastHashJoin [ZipCode#1159], [cast(ZCTA10#1055 as int)], Inner, BuildRight, false\n",
      "                           :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "                           :  +- Filter isnotnull(ZipCode#1159)\n",
      "                           :     +- FileScan csv [ZipCode#1159,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[2, string, true] as int) as bigint)),false), [plan_id=2110]\n",
      "                              +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                                 +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                                    +- Generate explode(features#1014), false, [features#1022]\n",
      "                                       +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                                          +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df_merge = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "result_df_merge = result_df_merge.join(\n",
    "    income_per_area_df.hint('merge'),\n",
    "    ST_Within(result_df_merge['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df_merge.show()\n",
    "\n",
    "result_df_merge.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ3Merge = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['IncomePerPerson ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2464, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2467]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "         :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "         :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "         +- ResolvedHint (strategy=shuffle_hash)\n",
      "            +- Filter (CITY#1036 = Los Angeles)\n",
      "               +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                  +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                     +- Project [features#1022]\n",
      "                        +- Generate explode(features#1014), false, [features#1022]\n",
      "                           +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, IncomePerPerson: double, CrimesPerPerson: double\n",
      "Sort [IncomePerPerson#2464 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2464, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2467]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "         :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "         :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "         +- ResolvedHint (strategy=shuffle_hash)\n",
      "            +- Filter (CITY#1036 = Los Angeles)\n",
      "               +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                  +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                     +- Project [features#1022]\n",
      "                        +- Generate explode(features#1014), false, [features#1022]\n",
      "                           +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [IncomePerPerson#2464 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2464, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2467]\n",
      "   +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "         :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2200]\n",
      "         :  +- Filter (((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND (NOT (LAT#1196 = 0.0) AND NOT (LON#1197 = 0.0))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "         +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "            +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int)), rightHint=(strategy=shuffle_hash)\n",
      "               :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "               :  +- Filter isnotnull(ZipCode#1159)\n",
      "               :     +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "               +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                  +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                     +- Generate explode(features#1014), [0], false, [features#1022]\n",
      "                        +- Project [features#1014]\n",
      "                           +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#2464 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#2464 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2207]\n",
      "      +- HashAggregate(keys=[COMM#1038], functions=[sum(Income#1165), sum(POP_2010#1047L), count(1)], output=[COMM#1038, IncomePerPerson#2464, CrimesPerPerson#2467])\n",
      "         +- Exchange hashpartitioning(COMM#1038, 200), ENSURE_REQUIREMENTS, [plan_id=2204]\n",
      "            +- HashAggregate(keys=[COMM#1038], functions=[partial_sum(Income#1165), partial_sum(POP_2010#1047L), partial_count(1)], output=[COMM#1038, sum#2474, sum#2475L, count#2476L])\n",
      "               +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "                  +- RangeJoin geom#2200: geometry, geometry#1025: geometry, WITHIN\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2200]\n",
      "                     :  +- Filter ((((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND NOT (LAT#1196 = 0.0)) AND NOT (LON#1197 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan csv [LAT#1196,LON#1197] Batched: false, DataFilters: [isnotnull(LAT#1196), isnotnull(LON#1197), NOT (LAT#1196 = 0.0), NOT (LON#1197 = 0.0), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:float,LON:float>\n",
      "                     +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "                        +- ShuffledHashJoin [ZipCode#1159], [cast(ZCTA10#1055 as int)], Inner, BuildRight\n",
      "                           :- Exchange hashpartitioning(ZipCode#1159, 200), ENSURE_REQUIREMENTS, [plan_id=2196]\n",
      "                           :  +- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "                           :     +- Filter isnotnull(ZipCode#1159)\n",
      "                           :        +- FileScan csv [ZipCode#1159,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                           +- Exchange hashpartitioning(cast(ZCTA10#1055 as int), 200), ENSURE_REQUIREMENTS, [plan_id=2197]\n",
      "                              +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                                 +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                                    +- Generate explode(features#1014), false, [features#1022]\n",
      "                                       +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                                          +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake|46.097391668899746|0.001780055400923...|\n",
      "|     Panorama City|59.679510369299315|0.001597238227038...|\n",
      "|     Baldwin Hills| 78.72397418970363|0.002119263022880266|\n",
      "| Little Bangladesh| 79.19429343714515|0.002037585700718...|\n",
      "|   Wilshire Center| 79.91559362366671|0.002108441605087...|\n",
      "|   University Park| 84.82775008029655|0.003672160928616...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.19512628794847|0.004395147375951249|\n",
      "|     Vermont Vista| 95.96195522120188|0.003189405043223...|\n",
      "|        South Park| 96.07853726620282|0.003380633455086159|\n",
      "|        Pico-Union|100.70963557402804|0.003261075176157...|\n",
      "|    Toluca Terrace|102.38366107576634|0.002111046847888953|\n",
      "|        West Adams|104.16825869722096|0.003524301019893139|\n",
      "|          Van Nuys|104.96980872032377|0.002409082925590...|\n",
      "|    Vernon Central|108.06714827739671|0.003725417785654...|\n",
      "|         Koreatown| 111.3956805264591| 0.00320934587109571|\n",
      "|           Central|111.73594532501643|0.003708236722048...|\n",
      "|    East Hollywood|112.66524150058802|0.002808127994863853|\n",
      "|       North Hills|116.05924532549375|0.002047854352339578|\n",
      "|   Harvard Heights|116.34242568549196|0.003234721578082767|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_hash'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_hush'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ3ShuffleHash = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['IncomePerPerson ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2765, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2768]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, IncomePerPerson: double, CrimesPerPerson: double\n",
      "Sort [IncomePerPerson#2765 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2765, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2768]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#1170, DateRptd#1171, DATEOCC#1172, TIMEOCC#1173, AREA#1174, AREANAME#1175, RptDistNo#1176, Part#1177, CrmCd#1178, Crm Cd Desc#1179, Mocodes#1180, Vict Age#1181, VictSex#1182, VictDescent#1183, PremisCd#1184, PremisDesc#1185, WeaponUsedCd#1186, WeaponDesc#1187, Status#1188, Status Desc#1189, CrmCd1#1190, CrmCd2#1191, CrmCd3#1192, CrmCd4#1193, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#1196 = cast(0 as float)) AND NOT (LON#1197 = cast(0 as float)))\n",
      "      :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "      +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "         +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "            :- Project [ZipCode#1159, Community#1160, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "            :  +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "            +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "               +- Filter (CITY#1036 = Los Angeles)\n",
      "                  +- Project [properties#1026.BG10 AS BG10#1031, properties#1026.BG10FIP10 AS BG10FIP10#1032, properties#1026.BG12 AS BG12#1033, properties#1026.CB10 AS CB10#1034, properties#1026.CEN_FIP13 AS CEN_FIP13#1035, properties#1026.CITY AS CITY#1036, properties#1026.CITYCOM AS CITYCOM#1037, properties#1026.COMM AS COMM#1038, properties#1026.CT10 AS CT10#1039, properties#1026.CT12 AS CT12#1040, properties#1026.CTCB10 AS CTCB10#1041, properties#1026.HD_2012 AS HD_2012#1042L, properties#1026.HD_NAME AS HD_NAME#1043, properties#1026.HOUSING10 AS HOUSING10#1044L, properties#1026.LA_FIP10 AS LA_FIP10#1045, properties#1026.OBJECTID AS OBJECTID#1046L, properties#1026.POP_2010 AS POP_2010#1047L, properties#1026.PUMA10 AS PUMA10#1048, properties#1026.SPA_2012 AS SPA_2012#1049L, properties#1026.SPA_NAME AS SPA_NAME#1050, properties#1026.SUP_DIST AS SUP_DIST#1051, properties#1026.SUP_LABEL AS SUP_LABEL#1052, properties#1026.ShapeSTArea AS ShapeSTArea#1053, properties#1026.ShapeSTLength AS ShapeSTLength#1054, ... 2 more fields]\n",
      "                     +- Project [features#1022.geometry AS geometry#1025, features#1022.properties AS properties#1026, features#1022.type AS type#1027]\n",
      "                        +- Project [features#1022]\n",
      "                           +- Generate explode(features#1014), false, [features#1022]\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [IncomePerPerson#2765 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#1038], [COMM#1038, (sum(Income#1165) / cast(sum(POP_2010#1047L) as double)) AS IncomePerPerson#2765, (cast(count(1) as double) / cast(sum(POP_2010#1047L) as double)) AS CrimesPerPerson#2768]\n",
      "   +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=shuffle_replicate_nl)\n",
      "         :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2501]\n",
      "         :  +- Filter (((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND (NOT (LAT#1196 = 0.0) AND NOT (LON#1197 = 0.0))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#1170,DateRptd#1171,DATEOCC#1172,TIMEOCC#1173,AREA#1174,AREANAME#1175,RptDistNo#1176,Part#1177,CrmCd#1178,Crm Cd Desc#1179,Mocodes#1180,Vict Age#1181,VictSex#1182,VictDescent#1183,PremisCd#1184,PremisDesc#1185,WeaponUsedCd#1186,WeaponDesc#1187,Status#1188,Status Desc#1189,CrmCd1#1190,CrmCd2#1191,CrmCd3#1192,CrmCd4#1193,... 4 more fields] csv\n",
      "         +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "            +- Join Inner, (ZipCode#1159 = cast(ZCTA10#1055 as int)), rightHint=(strategy=shuffle_replicate_nl)\n",
      "               :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "               :  +- Filter isnotnull(ZipCode#1159)\n",
      "               :     +- Relation [ZipCode#1159,Community#1160,Income#1161] csv\n",
      "               +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                  +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                     +- Generate explode(features#1014), [0], false, [features#1022]\n",
      "                        +- Project [features#1014]\n",
      "                           +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                              +- Relation [crs#1013,features#1014,name#1015,type#1016] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#2765 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#2765 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2561]\n",
      "      +- HashAggregate(keys=[COMM#1038], functions=[sum(Income#1165), sum(POP_2010#1047L), count(1)], output=[COMM#1038, IncomePerPerson#2765, CrimesPerPerson#2768])\n",
      "         +- Exchange hashpartitioning(COMM#1038, 200), ENSURE_REQUIREMENTS, [plan_id=2558]\n",
      "            +- HashAggregate(keys=[COMM#1038], functions=[partial_sum(Income#1165), partial_sum(POP_2010#1047L), partial_count(1)], output=[COMM#1038, sum#2775, sum#2776L, count#2777L])\n",
      "               +- Project [Income#1165, COMM#1038, POP_2010#1047L]\n",
      "                  +- RangeJoin geom#2501: geometry, geometry#1025: geometry, WITHIN\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2501]\n",
      "                     :  +- Filter ((((isnotnull(LAT#1196) AND isnotnull(LON#1197)) AND NOT (LAT#1196 = 0.0)) AND NOT (LON#1197 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan csv [LAT#1196,LON#1197] Batched: false, DataFilters: [isnotnull(LAT#1196), isnotnull(LON#1197), NOT (LAT#1196 = 0.0), NOT (LON#1197 = 0.0), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:float,LON:float>\n",
      "                     +- Project [Income#1165, COMM#1038, POP_2010#1047L, geometry#1025]\n",
      "                        +- CartesianProduct (ZipCode#1159 = cast(ZCTA10#1055 as int))\n",
      "                           :- Project [ZipCode#1159, cast(regexp_replace(Income#1161, [$,], , 1) as float) AS Income#1165]\n",
      "                           :  +- Filter isnotnull(ZipCode#1159)\n",
      "                           :     +- FileScan csv [ZipCode#1159,Income#1161] Batched: false, DataFilters: [isnotnull(ZipCode#1159)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                           +- Project [features#1022.properties.COMM AS COMM#1038, features#1022.properties.POP_2010 AS POP_2010#1047L, features#1022.properties.ZCTA10 AS ZCTA10#1055, features#1022.geometry AS geometry#1025]\n",
      "                              +- Filter ((isnotnull(features#1022.properties.CITY) AND (features#1022.properties.CITY = Los Angeles)) AND (isnotnull(features#1022.properties.ZCTA10) AND isnotnull(features#1022.geometry)))\n",
      "                                 +- Generate explode(features#1014), false, [features#1022]\n",
      "                                    +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "                                       +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake|46.097391668899746|0.001780055400923...|\n",
      "|     Panorama City|59.679510369299315|0.001597238227038...|\n",
      "|     Baldwin Hills| 78.72397418970363|0.002119263022880266|\n",
      "| Little Bangladesh| 79.19429343714515|0.002037585700718...|\n",
      "|   Wilshire Center| 79.91559362366671|0.002108441605087...|\n",
      "|   University Park| 84.82775008029655|0.003672160928616...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.19512628794847|0.004395147375951249|\n",
      "|     Vermont Vista| 95.96195522120188|0.003189405043223...|\n",
      "|        South Park| 96.07853726620282|0.003380633455086159|\n",
      "|        Pico-Union|100.70963557402804|0.003261075176157...|\n",
      "|    Toluca Terrace|102.38366107576634|0.002111046847888953|\n",
      "|        West Adams|104.16825869722096|0.003524301019893139|\n",
      "|          Van Nuys|104.96980872032377|0.002409082925590...|\n",
      "|    Vernon Central|108.06714827739671|0.003725417785654...|\n",
      "|         Koreatown| 111.3956805264591| 0.00320934587109571|\n",
      "|           Central|111.73594532501643|0.003708236722048...|\n",
      "|    East Hollywood|112.66524150058802|0.002808127994863853|\n",
      "|       North Hills|116.05924532549375|0.002047854352339578|\n",
      "|   Harvard Heights|116.34242568549196|0.003234721578082767|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_replicate_nl'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_replicate_nl'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ3ShuffleRepl = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times for each hint:\n",
      "Base: 35.44 sec\n",
      "Broadcast: 20.25 sec\n",
      "Merge: 30.78 sec\n",
      "Shuffle Hash: 31.43 sec\n",
      "Shuffle Replicate: 29.75 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Times for each hint:\n",
    "Base: {timeQ3Base:.2f} sec\n",
    "Broadcast: {timeQ3Broadcast:.2f} sec\n",
    "Merge: {timeQ3Merge:.2f} sec\n",
    "Shuffle Hash: {timeQ3ShuffleHash:.2f} sec\n",
    "Shuffle Replicate: {timeQ3ShuffleRepl:.2f} sec\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 4 χρησιμοποιώντας το DataFrame ή SQL API. Να εκτελέσετε την υλοποίησή σας εφαρμόζοντας κλιμάκωση στο σύνολο των υπολογιστικών πόρων που θα χρησιμοποιήσετε: Συγκεκριμένα, καλείστε να εκτελέστε την υλοποίησή σας σε 2 executors με τα ακόλουθα configurations:\n",
    "\n",
    "- 1 core/2 GB memory\n",
    "- 2 cores/4GB memory\n",
    "- 4 cores/8GB memory\n",
    "\n",
    "Σχολιάστε τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 core / 2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 1\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATEOCC', to_timestamp('DATEOCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATEOCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4435|\n",
      "|               Black| 2296|\n",
      "|               White|  956|\n",
      "|               Other|  931|\n",
      "|         Other Asian|  198|\n",
      "|             Unknown|  174|\n",
      "|              Korean|   38|\n",
      "|            Filipino|   21|\n",
      "|           Guamanian|    3|\n",
      "|             Chinese|    2|\n",
      "|            Japanese|    2|\n",
      "|American Indian/A...|    2|\n",
      "|    Pacific Islander|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ4_1 = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 cores / 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 2\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATEOCC', to_timestamp('DATEOCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATEOCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4435|\n",
      "|               Black| 2296|\n",
      "|               White|  956|\n",
      "|               Other|  931|\n",
      "|         Other Asian|  198|\n",
      "|             Unknown|  174|\n",
      "|              Korean|   38|\n",
      "|            Filipino|   21|\n",
      "|           Guamanian|    3|\n",
      "|             Chinese|    2|\n",
      "|            Japanese|    2|\n",
      "|    Pacific Islander|    2|\n",
      "|American Indian/A...|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ4_2 = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 cores / 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 3\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATEOCC', to_timestamp('DATEOCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATEOCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['VictDescent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4435|\n",
      "|               Black| 2296|\n",
      "|               White|  956|\n",
      "|               Other|  931|\n",
      "|         Other Asian|  198|\n",
      "|             Unknown|  174|\n",
      "|              Korean|   38|\n",
      "|            Filipino|   21|\n",
      "|           Guamanian|    3|\n",
      "|             Chinese|    2|\n",
      "|            Japanese|    2|\n",
      "|American Indian/A...|    2|\n",
      "|    Pacific Islander|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "timeQ4_3 = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times taken for each settings:\n",
      "- 1 core / 2 GB: 98.70 sec\n",
      "- 2 core / 4 GB: 99.62 sec\n",
      "- 4 core / 8 GB: 98.68 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Times taken for each settings:\n",
    "- 1 core / 2 GB: {timeQ4_1:.2f} sec\n",
    "- 2 core / 4 GB: {timeQ4_2:.2f} sec\n",
    "- 4 core / 8 GB: {timeQ4_3:.2f} sec\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Executors x 4 cores / 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = spark.newSession().builder \\\n",
    "    .appName(\"Query 5 Part 1\") \\\n",
    "    .config('spark.executor.instances', '2') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize SedonaContext\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "stations_geom_df = stations_df.withColumn('StationsGeom', ST_Point(col('Y'), col('X')))\n",
    "\n",
    "crimes_geom_df = crimes_df.filter((col('LAT') != 0) & (col('LON') != 0)) \\\n",
    "    .withColumn('CrimesGeom', ST_Point(col('LAT'), col('LON')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----------+\n",
      "|        Division|      avg_distance|num_crimes|\n",
      "+----------------+------------------+----------+\n",
      "|         OLYMPIC|1.5399445462398649|    153479|\n",
      "|        WILSHIRE| 2.248960033397254|    153129|\n",
      "|        VAN NUYS| 2.116039225369137|    141765|\n",
      "|       SOUTHWEST|1.7235245838416613|    135620|\n",
      "| NORTH HOLLYWOOD| 2.236062149803519|    129372|\n",
      "|       HOLLYWOOD|1.7622581755231947|    112887|\n",
      "|     77TH STREET| 1.223632677540864|    111430|\n",
      "|      HOLLENBECK| 2.614000796608533|    111427|\n",
      "|         TOPANGA| 2.373388689781062|    108598|\n",
      "|         RAMPART|1.0616486612138145|     99284|\n",
      "|       SOUTHEAST|1.9159889369204468|     95160|\n",
      "|         PACIFIC| 2.777580705804208|     92999|\n",
      "|         CENTRAL|0.7766393931203354|     91676|\n",
      "|WEST LOS ANGELES| 2.470498404763809|     89606|\n",
      "|          HARBOR|  2.42732704709091|     88494|\n",
      "|     WEST VALLEY| 2.518734377998014|     88196|\n",
      "|         MISSION|2.4712537747545213|     82504|\n",
      "|        FOOTHILL|2.1577302676037466|     71676|\n",
      "|          NEWTON|1.4578837490511187|     65175|\n",
      "|      DEVONSHIRE|2.4452213070445934|     55447|\n",
      "|       NORTHEAST|3.3268069543588394|     44212|\n",
      "+----------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "time_start = time.time()\n",
    "\n",
    "# Perform a cross join to calculate distances between all crimes and stations\n",
    "distance_df = crimes_geom_df.crossJoin(stations_geom_df) \\\n",
    "    .withColumn(\n",
    "        'distance',\n",
    "        (ST_DistanceSphere('CrimesGeom', 'StationsGeom') / 1000)\n",
    "    )\n",
    "\n",
    "# Find the closest station for each crime\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"distance\")\n",
    "closest_station_df = distance_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1)  # Keep only the closest station\n",
    "\n",
    "# Aggregate results by station\n",
    "result_df = closest_station_df.groupBy(\"Division\") \\\n",
    "    .agg(\n",
    "        avg(\"distance\").alias(\"avg_distance\"),  # Average distance of crimes to the station\n",
    "        count(\"*\").alias(\"num_crimes\")  # Total crimes closest to the station\n",
    "    ) \\\n",
    "    .orderBy(desc(\"num_crimes\"))  # Sort by number of crimes in descending order\n",
    "\n",
    "# Display the results\n",
    "result_df.show(21)\n",
    "\n",
    "# End timer\n",
    "time_end = time.time()\n",
    "timeQ5_1 = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Executors x 2 cores / 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = spark.newSession().builder \\\n",
    "    .appName(\"Query 5 Part 2\") \\\n",
    "    .config('spark.executor.instances', '4') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize SedonaContext\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "stations_geom_df = stations_df.withColumn('StationsGeom', ST_Point(col('Y'), col('X')))\n",
    "\n",
    "crimes_geom_df = crimes_df.filter((col('LAT') != 0) & (col('LON') != 0)) \\\n",
    "    .withColumn('CrimesGeom', ST_Point(col('LAT'), col('LON')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----------+\n",
      "|        Division|      avg_distance|num_crimes|\n",
      "+----------------+------------------+----------+\n",
      "|         OLYMPIC|1.5399445462398649|    153479|\n",
      "|        WILSHIRE| 2.248960033397254|    153129|\n",
      "|        VAN NUYS| 2.116039225369137|    141765|\n",
      "|       SOUTHWEST|1.7235245838416613|    135620|\n",
      "| NORTH HOLLYWOOD| 2.236062149803519|    129372|\n",
      "|       HOLLYWOOD|1.7622581755231947|    112887|\n",
      "|     77TH STREET| 1.223632677540864|    111430|\n",
      "|      HOLLENBECK| 2.614000796608533|    111427|\n",
      "|         TOPANGA| 2.373388689781062|    108598|\n",
      "|         RAMPART|1.0616486612138145|     99284|\n",
      "|       SOUTHEAST|1.9159889369204468|     95160|\n",
      "|         PACIFIC| 2.777580705804208|     92999|\n",
      "|         CENTRAL|0.7766393931203354|     91676|\n",
      "|WEST LOS ANGELES| 2.470498404763809|     89606|\n",
      "|          HARBOR|  2.42732704709091|     88494|\n",
      "|     WEST VALLEY| 2.518734377998014|     88196|\n",
      "|         MISSION|2.4712537747545213|     82504|\n",
      "|        FOOTHILL|2.1577302676037466|     71676|\n",
      "|          NEWTON|1.4578837490511187|     65175|\n",
      "|      DEVONSHIRE|2.4452213070445934|     55447|\n",
      "|       NORTHEAST|3.3268069543588394|     44212|\n",
      "+----------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "time_start = time.time()\n",
    "\n",
    "# Perform a cross join to calculate distances between all crimes and stations\n",
    "distance_df = crimes_geom_df.crossJoin(stations_geom_df) \\\n",
    "    .withColumn(\n",
    "        'distance',\n",
    "        (ST_DistanceSphere('CrimesGeom', 'StationsGeom') / 1000)\n",
    "    )\n",
    "\n",
    "# Find the closest station for each crime\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"distance\")\n",
    "closest_station_df = distance_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1)  # Keep only the closest station\n",
    "\n",
    "# Aggregate results by station\n",
    "result_df = closest_station_df.groupBy(\"Division\") \\\n",
    "    .agg(\n",
    "        avg(\"distance\").alias(\"avg_distance\"),  # Average distance of crimes to the station\n",
    "        count(\"*\").alias(\"num_crimes\")  # Total crimes closest to the station\n",
    "    ) \\\n",
    "    .orderBy(desc(\"num_crimes\"))  # Sort by number of crimes in descending order\n",
    "\n",
    "# Display the results\n",
    "result_df.show(21)\n",
    "\n",
    "# End timer\n",
    "time_end = time.time()\n",
    "timeQ5_2 = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Executors x 1 cores / 2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = spark.newSession().builder \\\n",
    "    .appName(\"Query 5 Part 3\") \\\n",
    "    .config('spark.executor.instances', '8') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize SedonaContext\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "stations_geom_df = stations_df.withColumn('StationsGeom', ST_Point(col('Y'), col('X')))\n",
    "\n",
    "crimes_geom_df = crimes_df.filter((col('LAT') != 0) & (col('LON') != 0)) \\\n",
    "    .withColumn('CrimesGeom', ST_Point(col('LAT'), col('LON')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----------+\n",
      "|        Division|      avg_distance|num_crimes|\n",
      "+----------------+------------------+----------+\n",
      "|         OLYMPIC|1.5399445462398649|    153479|\n",
      "|        WILSHIRE| 2.248960033397254|    153129|\n",
      "|        VAN NUYS| 2.116039225369137|    141765|\n",
      "|       SOUTHWEST|1.7235245838416613|    135620|\n",
      "| NORTH HOLLYWOOD| 2.236062149803519|    129372|\n",
      "|       HOLLYWOOD|1.7622581755231947|    112887|\n",
      "|     77TH STREET| 1.223632677540864|    111430|\n",
      "|      HOLLENBECK| 2.614000796608533|    111427|\n",
      "|         TOPANGA| 2.373388689781062|    108598|\n",
      "|         RAMPART|1.0616486612138145|     99284|\n",
      "|       SOUTHEAST|1.9159889369204468|     95160|\n",
      "|         PACIFIC| 2.777580705804208|     92999|\n",
      "|         CENTRAL|0.7766393931203354|     91676|\n",
      "|WEST LOS ANGELES| 2.470498404763809|     89606|\n",
      "|          HARBOR|  2.42732704709091|     88494|\n",
      "|     WEST VALLEY| 2.518734377998014|     88196|\n",
      "|         MISSION|2.4712537747545213|     82504|\n",
      "|        FOOTHILL|2.1577302676037466|     71676|\n",
      "|          NEWTON|1.4578837490511187|     65175|\n",
      "|      DEVONSHIRE|2.4452213070445934|     55447|\n",
      "|       NORTHEAST|3.3268069543588394|     44212|\n",
      "+----------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "time_start = time.time()\n",
    "\n",
    "# Perform a cross join to calculate distances between all crimes and stations\n",
    "distance_df = crimes_geom_df.crossJoin(stations_geom_df) \\\n",
    "    .withColumn(\n",
    "        'distance',\n",
    "        (ST_DistanceSphere('CrimesGeom', 'StationsGeom') / 1000)\n",
    "    )\n",
    "\n",
    "# Find the closest station for each crime\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"distance\")\n",
    "closest_station_df = distance_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1)  # Keep only the closest station\n",
    "\n",
    "# Aggregate results by station\n",
    "result_df = closest_station_df.groupBy(\"Division\") \\\n",
    "    .agg(\n",
    "        avg(\"distance\").alias(\"avg_distance\"),  # Average distance of crimes to the station\n",
    "        count(\"*\").alias(\"num_crimes\")  # Total crimes closest to the station\n",
    "    ) \\\n",
    "    .orderBy(desc(\"num_crimes\"))  # Sort by number of crimes in descending order\n",
    "\n",
    "# Display the results\n",
    "result_df.show(21)\n",
    "\n",
    "# End timer\n",
    "time_end = time.time()\n",
    "timeQ5_3 = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times for each case:\n",
      "Executors: 2, Cores: 4, Memory: 8 GB, Time: 27.01\n",
      "Executors: 4, Cores: 2, Memory: 4 GB, Time: 26.08\n",
      "Executors: 8, Cores: 1, Memory: 2 GB, Time: 27.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Times for each case:\n",
    "Executors: 2, Cores: 4, Memory: 8 GB, Time: {timeQ5_1:.2f}\n",
    "Executors: 4, Cores: 2, Memory: 4 GB, Time: {timeQ5_2:.2f}\n",
    "Executors: 8, Cores: 1, Memory: 2 GB, Time: {timeQ5_3:.2f}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
