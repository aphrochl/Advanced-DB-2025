{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Περιεχόμενα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to be used\n",
    "\n",
    "# Paths for csv\n",
    "fcrime = \"..\\Data\\Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "fstations = \"..\\Data\\LA_Police_Stations.csv\"\n",
    "fincome = \"..\\Data\\LA_income_2015.csv\"\n",
    "fcodes = \"..\\Data\\RE_codes.csv\"\n",
    "\n",
    "# Paths for .parquet\n",
    "fcrime_parq = \"..\\Data\\CrimeData.parquet\"\n",
    "fstations_parq = \"..\\Data\\PoliceStations.parquet\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"../Data/2010_Census_Blocks.geojson\"\n",
    "fgeofields = \"../Data/2010_Census_Blocks_fields.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF/SQL API\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", DateType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Να υλοποιηθεί το __Query 1__ χρησιμοποιώντας τα DataFrame και RDD APIs. Να εκτελέσετε και τις δύο υλοποιήσεις με 4 Spark executors. Υπάρχει διαφορά στην επίδοση μεταξύ των δύο APIs; Αιτιολογήσετε την απάντησή σας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize into age groups\n",
    "categorized_df = crimes_df.withColumn('age_group',\n",
    "                    when(col('VictAge').cast('int') < 18, 'Children')\n",
    "                    .when(\n",
    "                        ((col('VictAge').cast('int') >= 18) & (col('VictAge').cast('int') <= 24)), 'Young Adults'\n",
    "                        )\n",
    "                    .when(\n",
    "                        ((col('VictAge').cast('int') >= 25) & (col('VictAge').cast('int') <= 64)), 'Adults'\n",
    "                        )\n",
    "                    .when(col('VictAge').cast('int') > 64, 'Elderly')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'AGGRAVATED ASSAULT'\n",
    "assault_df = categorized_df.filter(\n",
    "    col('CrmCdDesc').contains('AGGRAVATED ASSAULT')\n",
    "    ) \\\n",
    "    .groupby('age_group') \\\n",
    "    .agg(count('*').alias('victim_count')) \\\n",
    "    .orderBy(col('victim_count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|   age_group|victim_count|\n",
      "+------------+------------+\n",
      "|      Adults|       72610|\n",
      "|Young Adults|       23472|\n",
      "|    Children|       10724|\n",
      "|     Elderly|        3099|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "assault_df.show()\n",
    "\n",
    "time_end = time.time()\n",
    "time_df = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δυστυχώς στα δεδομένα μας έχουμε μερικές περιπτώσεις όπου υπάρχει υποδιαστολή εντός quote marks (\"\"), δημιουργώντας έτσι θέμα στο parse.\n",
    "Για αυτό θα χρησιμοποιήσουμε μια βιβλιοθήκη της Python για την ανάγνωση των csv αρχείων και θα την περάσουμε μέσω mapping σε κάθε δεδομένο:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_line(line):\n",
    "    reader = csv.reader([line])\n",
    "    fields = next(reader)\n",
    "    return {\n",
    "        'DR_NO': fields[0],\n",
    "        'DateRptd': fields[1],\n",
    "        'DATEOCC': fields[2],\n",
    "        'TIMEOCC': fields[3],\n",
    "        'AREA': fields[4],\n",
    "        'AREANAME': fields[5],\n",
    "        'RptDistNo': fields[6],\n",
    "        'Part': fields[7],\n",
    "        'CrmCd': fields[8],\n",
    "        'CrmCdDesc': fields[9],\n",
    "        'Mocodes': fields[10],\n",
    "        'VictAge': int(fields[11]),\n",
    "        'VictSex': fields[12],\n",
    "        'VictDescent': fields[13],\n",
    "        'PremisCd': fields[14],\n",
    "        'PremisDesc': fields[15],\n",
    "        'WeaponUsedCd': fields[16],\n",
    "        'WeaponDesc': fields[17],\n",
    "        'Status': fields[18],\n",
    "        'CrmCd1': fields[19],\n",
    "        'CrmCd2': fields[20],\n",
    "        'CrmCd3': fields[21],\n",
    "        'CrmCd4': fields[22],\n",
    "        'LOCATION': fields[23],\n",
    "        'CrossStreet': fields[24],\n",
    "        'LAT': fields[25],\n",
    "        'LON': fields[26]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv as Text\n",
    "rdd = spark.sparkContext.textFile(fcrime)\n",
    "\n",
    "# Remove the header\n",
    "header = rdd.first()\n",
    "crimes_rdd = rdd.filter(lambda line: line != header).map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function to categorize age groups\n",
    "def categorize_age(crime):\n",
    "    age = crime['VictAge']\n",
    "    if age < 18:\n",
    "        return 'Children'\n",
    "    elif age >= 18 and age <= 24:\n",
    "        return 'Young Adults'\n",
    "    elif age >= 25 and age <= 64:\n",
    "        return 'Adults'\n",
    "    else:\n",
    "        return 'Elderly'\n",
    "    \n",
    "categorized_rdd = crimes_rdd.filter(lambda x: 'AGGRAVATED ASSAULT' in x['CrmCdDesc']) \\\n",
    "                    .map(lambda x: (categorize_age(x), 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b) \\\n",
    "                    .sortBy(lambda x: -x[1])\n",
    "\n",
    "categorized_rdd.collect()\n",
    "time_end = time.time()\n",
    "time_rdd = time_end - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παρατηρήσεις:\n",
    "\n",
    "Η διαφορά στην απόδοση μεταξύ των 2 μεθόδων δεν είναι τόσο εμφανής όσο αναμέναμε.\n",
    "Τα DataFrames αξιοποιούν το optimization και προσφέρουν γενικά μεγαλύτερη ταχύτητα, αν και στη συγκεκριμένη περίπτωση η διαφορά είναι μικρή.\n",
    "Αντιθέτως, τα RDD προσφέρουν μεγαλύτερη ευελιξία, αφού είναι low level, στην επεξεργασία των δεδομένων. \n",
    "\n",
    "Πιθανόν το μέγεθος των δεδομένων να μην είναι αρκετά μεγάλο ώστε να αρχίσει να φαίνεται μια ουσιαστική διαφορά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for DataFrame API: 8.369946956634521.\n",
      "Time taken for RDD API: 83.3004641532898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Time taken for DataFrame API: {time_df}.\n",
    "Time taken for RDD API: {time_rdd}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το __Query 2__ χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    crimes_df.AREA.cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(crimes_df.DateRptd).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table rsults\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of DF spark session to completion: 4.93 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time = time_end - time_start\n",
    "print(f'Time taken since creation of DF spark session to completion: {df_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes Table\n",
    "\n",
    "crimes_schema_sql = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", StringType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.format('csv') \\\n",
    "    .options(header='true', dateFormat='MM/dd/yyyy hh:mm:ss a') \\\n",
    "    .schema(crimes_schema_sql) \\\n",
    "    .load(fcrime)\n",
    "\n",
    "crimes_df = crimes_df.withColumn(\"DateRptd\", to_timestamp(\"DateRptd\", \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "                     .withColumn(\"DATEOCC\", to_timestamp(\"DATEOCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations Table\n",
    "\n",
    "stations_schema_sql = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(stations_schema_sql) \\\n",
    "    .load(fstations)\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql = \"\"\"WITH YearlyPrecinctStats AS ( \n",
    "    SELECT \n",
    "        YEAR(c.DateRptd) AS year,\n",
    "        s.DIVISION AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS closed_case_rate\n",
    "    FROM crimes c\n",
    "    JOIN stations s\n",
    "        ON CAST(c.AREA as INTEGER) = s.FID\n",
    "    GROUP BY YEAR(c.DateRptd), s.DIVISION\n",
    "    ),\n",
    "    rankedPrecincts AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM YearlyPrecinctStats\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM RankedPrecincts\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY year, ranking;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__ \n",
    "\n",
    "YearlyPrecinctStats\n",
    "- We need to group our data by year and department\n",
    "- Keep a count of all cases and a count of closed cases\n",
    "- Create the rate as a percentage\n",
    "\n",
    "RankedPrecincts\n",
    "- From YearlyPrecinctStats keep: year, precinct and closed_case_rate\n",
    "- We will need to create the ranking based on the closed cases rate of each department\n",
    "- For each year assign a ranking (starting at 1) in descending order\n",
    "\n",
    "Notes:\n",
    "\n",
    "The symbol '#' is not supported in SQL so it was changed to ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+\n",
      "|year|  precinct| closed_case_rate|ranking|\n",
      "+----+----------+-----------------+-------+\n",
      "|2010| SOUTHEAST|32.94735585531813|      1|\n",
      "|2010|DEVONSHIRE|31.96270619172842|      2|\n",
      "|2010| SOUTHWEST|29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.21216768916155|      1|\n",
      "|2011| SOUTHEAST|32.51177963030083|      2|\n",
      "|2011| SOUTHWEST|28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.41481831052383|      1|\n",
      "|2012| SOUTHEAST|32.94641810294290|      2|\n",
      "|2012| SOUTHWEST|29.81513327601032|      3|\n",
      "|2013|DEVONSHIRE|33.52812271731191|      1|\n",
      "|2013| SOUTHEAST|32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.16422459266206|      3|\n",
      "|2014|HOLLENBECK|31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.31198995605775|      2|\n",
      "|2014|  FOOTHILL|31.16279069767442|      3|\n",
      "|2015|HOLLENBECK|32.64134698172773|      1|\n",
      "|2015|  WILSHIRE|30.27597402597403|      2|\n",
      "|2015|  FOOTHILL|30.17946067838016|      3|\n",
      "|2016|HOLLENBECK|31.88075572011773|      1|\n",
      "|2016|  WILSHIRE|31.54798761609907|      2|\n",
      "+----+----------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of spark session to query completion: 4.55 seconds\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query2_sql).show()\n",
    "time_end = time.time()\n",
    "sql_time = time_end - time_start\n",
    "print(f\"Time taken since creation of spark session to query completion: {sql_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for DataFrame API: 4.93\n",
      "Time taken for SQL API: 4.55\n"
     ]
    }
   ],
   "source": [
    "print(f'''Time taken for DataFrame API: {df_time:.2f}\n",
    "Time taken for SQL API: {sql_time:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι οι χρόνοι εκτέλεσης δεν παρουσιάζουν μεγάλες αποκλίσεις.\n",
    "Παραδόξως, οι χρόνοι εκτέλεσης για το SQL API είναι συνήθως σταθεροί για όσες φορές τρέξουμε τον κώδικα, ενώ στο DataFrame API παρατηρούμε μεγάλη απόκλιση μεταξύ των τιμών για πολλαπλές εκτελέσεις του κώδικα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .config(\"spark.hadoop.fs.hdfs.impl.disable.cache\", \"true\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\") \\\n",
    "    .appName(\"Query 2 Part 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv(fcrime,header=True, inferSchema=True)\n",
    "crimes_df.write.mode('overwrite').parquet(fcrime_parq)\n",
    "\n",
    "stations_df = spark.read.csv(fstations,header=True, inferSchema=True)\n",
    "stations_df.write.mode('overwrite').parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιήσουμε το DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.parquet(fcrime_parq)\n",
    "stations_df = spark.read.parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = crimes_df.withColumn('Date Rptd', to_timestamp('Date Rptd', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    col('AREA ').cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(col('Date Rptd')).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table results\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for DF with parquet to completion: 6.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time_parq = time_end - time_start\n",
    "print(f'Time taken for DF with parquet to completion: {df_time_parq:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας τα parquet δεδομένα, παρατηρούμε σταθερά μια αρκετά μεγάλη βελτίωση στην ταχύτητα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 3 χρησιμοποιώντας DataFrame ή SQL API. Χρησιμοποιήστε τις μεθόδους hint & explain για να βρείτε ποιες στρατηγικές join χρησιμοποιεί ο catalyst optimizer.\n",
    "Πειραματιστείτε αναγκάζοντας το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές (μεταξύ\n",
    "των BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL) και σχολιάστε τα αποτελέσματα\n",
    "που παρατηρείτε. Ποιά (ή ποιές) από τις διαθέσιμες στρατηγικές join του Spark είναι καταλληλότερη(ες) και γιατί;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_schema = StructType([\n",
    "    StructField('ZipCode', IntegerType()),\n",
    "    StructField('Community', StringType()),\n",
    "    StructField('Income', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = spark.read.csv(fincome, header=True, schema=income_schema)\n",
    "\n",
    "# Remove the $ character\n",
    "income_df = income_df.withColumn(\n",
    "    'Income', \n",
    "    regexp_replace(col('Income'), r\"[$,]\", \"\").cast('float')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_flattened_df = flattened_df.filter(col('CITY') == 'Los Angeles')\n",
    "crimes_df = crimes_df.filter(\n",
    "    (col('LAT') != 0) & (col('LON') != 0)\n",
    ")\n",
    "\n",
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_per_area_df = income_df.join(la_flattened_df, col('ZipCode') == la_flattened_df['ZCTA10'])\n",
    "\n",
    "# Table for Query 4\n",
    "q4 = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry'])\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df,\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake| 47.06267163013844|0.001818971610681...|\n",
      "|     Panorama City| 59.95984645959698|0.001604741028536...|\n",
      "| Little Bangladesh|  77.4462882372383|0.001979373403452...|\n",
      "|     Baldwin Hills| 78.78636074000164|0.002120942479621...|\n",
      "|   Wilshire Center| 81.12746576217563|0.002112811398485...|\n",
      "|   University Park|  84.2864598957781|0.003648016379738...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.40555901521596|0.004404426023303084|\n",
      "|     Vermont Vista| 95.87913381880738|0.003186652379442434|\n",
      "|        South Park| 96.21397332574081|0.003385019532433...|\n",
      "|        Pico-Union|101.48672177250172|0.003285527032422...|\n",
      "|    Toluca Terrace|102.67364930834147|0.002117026109988...|\n",
      "|        West Adams| 104.8323250501383|0.003550872219509559|\n",
      "|          Van Nuys|105.51128525034808|0.002421671095030...|\n",
      "|    Vernon Central|106.84761010812721|0.003680139177439...|\n",
      "|         Koreatown|108.28659873027459| 0.00312340924152636|\n",
      "|           Central|108.98106070862619|0.003616755441572...|\n",
      "|    East Hollywood|112.86302381443797|0.002813818785432924|\n",
      "|       North Hills|114.96038575822709|0.002028475704098...|\n",
      "|   Harvard Heights|116.04382406428836|0.003217299281562349|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['IncomePerPerson ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#545], [COMM#545, (sum(Income#678) / cast(sum(POP_2010#554L) as double)) AS IncomePerPerson#1362, (cast(count(1) as double) / cast(sum(POP_2010#554L) as double)) AS CrimesPerPerson#1365]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#236, Date Rptd#304, DATE OCC#238, TIME OCC#239, AREA #240, AREA NAME#241, Rpt Dist No#242, Part 1-2#243, Crm Cd#244, Crm Cd Desc#245, Mocodes#246, Vict Age#247, Vict Sex#248, Vict Descent#249, Premis Cd#250, Premis Desc#251, Weapon Used Cd#252, Weapon Desc#253, Status#254, Status Desc#255, Crm Cd 1#256, Crm Cd 2#257, Crm Cd 3#258, Crm Cd 4#259, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#262 = cast(0 as double)) AND NOT (LON#263 = cast(0 as double)))\n",
      "      :     +- Project [DR_NO#236, to_timestamp(Date Rptd#237, Some(MM/dd/yyyy hh:mm:ss a), TimestampType, Some(Europe/Athens), false) AS Date Rptd#304, DATE OCC#238, TIME OCC#239, AREA #240, AREA NAME#241, Rpt Dist No#242, Part 1-2#243, Crm Cd#244, Crm Cd Desc#245, Mocodes#246, Vict Age#247, Vict Sex#248, Vict Descent#249, Premis Cd#250, Premis Desc#251, Weapon Used Cd#252, Weapon Desc#253, Status#254, Status Desc#255, Crm Cd 1#256, Crm Cd 2#257, Crm Cd 3#258, Crm Cd 4#259, ... 4 more fields]\n",
      "      :        +- Relation [DR_NO#236,Date Rptd#237,DATE OCC#238,TIME OCC#239,AREA #240,AREA NAME#241,Rpt Dist No#242,Part 1-2#243,Crm Cd#244,Crm Cd Desc#245,Mocodes#246,Vict Age#247,Vict Sex#248,Vict Descent#249,Premis Cd#250,Premis Desc#251,Weapon Used Cd#252,Weapon Desc#253,Status#254,Status Desc#255,Crm Cd 1#256,Crm Cd 2#257,Crm Cd 3#258,Crm Cd 4#259,... 4 more fields] parquet\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Join Inner, (ZipCode#672 = cast(ZCTA10#562 as int))\n",
      "            :- Project [ZipCode#672, Community#673, cast(regexp_replace(Income#674, [$,], , 1) as float) AS Income#678]\n",
      "            :  +- Relation [ZipCode#672,Community#673,Income#674] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#543 = Los Angeles)\n",
      "                  +- Project [properties#533.BG10 AS BG10#538, properties#533.BG10FIP10 AS BG10FIP10#539, properties#533.BG12 AS BG12#540, properties#533.CB10 AS CB10#541, properties#533.CEN_FIP13 AS CEN_FIP13#542, properties#533.CITY AS CITY#543, properties#533.CITYCOM AS CITYCOM#544, properties#533.COMM AS COMM#545, properties#533.CT10 AS CT10#546, properties#533.CT12 AS CT12#547, properties#533.CTCB10 AS CTCB10#548, properties#533.HD_2012 AS HD_2012#549L, properties#533.HD_NAME AS HD_NAME#550, properties#533.HOUSING10 AS HOUSING10#551L, properties#533.LA_FIP10 AS LA_FIP10#552, properties#533.OBJECTID AS OBJECTID#553L, properties#533.POP_2010 AS POP_2010#554L, properties#533.PUMA10 AS PUMA10#555, properties#533.SPA_2012 AS SPA_2012#556L, properties#533.SPA_NAME AS SPA_NAME#557, properties#533.SUP_DIST AS SUP_DIST#558, properties#533.SUP_LABEL AS SUP_LABEL#559, properties#533.ShapeSTArea AS ShapeSTArea#560, properties#533.ShapeSTLength AS ShapeSTLength#561, ... 2 more fields]\n",
      "                     +- Project [features#529.geometry AS geometry#532, features#529.properties AS properties#533, features#529.type AS type#534]\n",
      "                        +- Project [features#529]\n",
      "                           +- Generate explode(features#521), false, [features#529]\n",
      "                              +- Relation [crs#520,features#521,name#522,type#523] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, IncomePerPerson: double, CrimesPerPerson: double\n",
      "Sort [IncomePerPerson#1362 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#545], [COMM#545, (sum(Income#678) / cast(sum(POP_2010#554L) as double)) AS IncomePerPerson#1362, (cast(count(1) as double) / cast(sum(POP_2010#554L) as double)) AS CrimesPerPerson#1365]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#236, Date Rptd#304, DATE OCC#238, TIME OCC#239, AREA #240, AREA NAME#241, Rpt Dist No#242, Part 1-2#243, Crm Cd#244, Crm Cd Desc#245, Mocodes#246, Vict Age#247, Vict Sex#248, Vict Descent#249, Premis Cd#250, Premis Desc#251, Weapon Used Cd#252, Weapon Desc#253, Status#254, Status Desc#255, Crm Cd 1#256, Crm Cd 2#257, Crm Cd 3#258, Crm Cd 4#259, ... 5 more fields]\n",
      "      :  +- Filter (NOT (LAT#262 = cast(0 as double)) AND NOT (LON#263 = cast(0 as double)))\n",
      "      :     +- Project [DR_NO#236, to_timestamp(Date Rptd#237, Some(MM/dd/yyyy hh:mm:ss a), TimestampType, Some(Europe/Athens), false) AS Date Rptd#304, DATE OCC#238, TIME OCC#239, AREA #240, AREA NAME#241, Rpt Dist No#242, Part 1-2#243, Crm Cd#244, Crm Cd Desc#245, Mocodes#246, Vict Age#247, Vict Sex#248, Vict Descent#249, Premis Cd#250, Premis Desc#251, Weapon Used Cd#252, Weapon Desc#253, Status#254, Status Desc#255, Crm Cd 1#256, Crm Cd 2#257, Crm Cd 3#258, Crm Cd 4#259, ... 4 more fields]\n",
      "      :        +- Relation [DR_NO#236,Date Rptd#237,DATE OCC#238,TIME OCC#239,AREA #240,AREA NAME#241,Rpt Dist No#242,Part 1-2#243,Crm Cd#244,Crm Cd Desc#245,Mocodes#246,Vict Age#247,Vict Sex#248,Vict Descent#249,Premis Cd#250,Premis Desc#251,Weapon Used Cd#252,Weapon Desc#253,Status#254,Status Desc#255,Crm Cd 1#256,Crm Cd 2#257,Crm Cd 3#258,Crm Cd 4#259,... 4 more fields] parquet\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Join Inner, (ZipCode#672 = cast(ZCTA10#562 as int))\n",
      "            :- Project [ZipCode#672, Community#673, cast(regexp_replace(Income#674, [$,], , 1) as float) AS Income#678]\n",
      "            :  +- Relation [ZipCode#672,Community#673,Income#674] csv\n",
      "            +- ResolvedHint (strategy=broadcast)\n",
      "               +- Filter (CITY#543 = Los Angeles)\n",
      "                  +- Project [properties#533.BG10 AS BG10#538, properties#533.BG10FIP10 AS BG10FIP10#539, properties#533.BG12 AS BG12#540, properties#533.CB10 AS CB10#541, properties#533.CEN_FIP13 AS CEN_FIP13#542, properties#533.CITY AS CITY#543, properties#533.CITYCOM AS CITYCOM#544, properties#533.COMM AS COMM#545, properties#533.CT10 AS CT10#546, properties#533.CT12 AS CT12#547, properties#533.CTCB10 AS CTCB10#548, properties#533.HD_2012 AS HD_2012#549L, properties#533.HD_NAME AS HD_NAME#550, properties#533.HOUSING10 AS HOUSING10#551L, properties#533.LA_FIP10 AS LA_FIP10#552, properties#533.OBJECTID AS OBJECTID#553L, properties#533.POP_2010 AS POP_2010#554L, properties#533.PUMA10 AS PUMA10#555, properties#533.SPA_2012 AS SPA_2012#556L, properties#533.SPA_NAME AS SPA_NAME#557, properties#533.SUP_DIST AS SUP_DIST#558, properties#533.SUP_LABEL AS SUP_LABEL#559, properties#533.ShapeSTArea AS ShapeSTArea#560, properties#533.ShapeSTLength AS ShapeSTLength#561, ... 2 more fields]\n",
      "                     +- Project [features#529.geometry AS geometry#532, features#529.properties AS properties#533, features#529.type AS type#534]\n",
      "                        +- Project [features#529]\n",
      "                           +- Generate explode(features#521), false, [features#529]\n",
      "                              +- Relation [crs#520,features#521,name#522,type#523] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [IncomePerPerson#1362 ASC NULLS FIRST], true\n",
      "+- Aggregate [COMM#545], [COMM#545, (sum(Income#678) / cast(sum(POP_2010#554L) as double)) AS IncomePerPerson#1362, (cast(count(1) as double) / cast(sum(POP_2010#554L) as double)) AS CrimesPerPerson#1365]\n",
      "   +- Project [Income#678, COMM#545, POP_2010#554L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=broadcast)\n",
      "         :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1098]\n",
      "         :  +- Filter (((isnotnull(LAT#262) AND isnotnull(LON#263)) AND (NOT (LAT#262 = 0.0) AND NOT (LON#263 = 0.0))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#236,Date Rptd#237,DATE OCC#238,TIME OCC#239,AREA #240,AREA NAME#241,Rpt Dist No#242,Part 1-2#243,Crm Cd#244,Crm Cd Desc#245,Mocodes#246,Vict Age#247,Vict Sex#248,Vict Descent#249,Premis Cd#250,Premis Desc#251,Weapon Used Cd#252,Weapon Desc#253,Status#254,Status Desc#255,Crm Cd 1#256,Crm Cd 2#257,Crm Cd 3#258,Crm Cd 4#259,... 4 more fields] parquet\n",
      "         +- Project [Income#678, COMM#545, POP_2010#554L, geometry#532]\n",
      "            +- Join Inner, (ZipCode#672 = cast(ZCTA10#562 as int)), rightHint=(strategy=broadcast)\n",
      "               :- Project [ZipCode#672, cast(regexp_replace(Income#674, [$,], , 1) as float) AS Income#678]\n",
      "               :  +- Filter isnotnull(ZipCode#672)\n",
      "               :     +- Relation [ZipCode#672,Community#673,Income#674] csv\n",
      "               +- Project [features#529.properties.COMM AS COMM#545, features#529.properties.POP_2010 AS POP_2010#554L, features#529.properties.ZCTA10 AS ZCTA10#562, features#529.geometry AS geometry#532]\n",
      "                  +- Filter ((isnotnull(features#529.properties.CITY) AND (features#529.properties.CITY = Los Angeles)) AND (isnotnull(features#529.properties.ZCTA10) AND isnotnull(features#529.geometry)))\n",
      "                     +- Generate explode(features#521), [0], false, [features#529]\n",
      "                        +- Project [features#521]\n",
      "                           +- Filter ((size(features#521, true) > 0) AND isnotnull(features#521))\n",
      "                              +- Relation [crs#520,features#521,name#522,type#523] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [IncomePerPerson#1362 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(IncomePerPerson#1362 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=785]\n",
      "      +- HashAggregate(keys=[COMM#545], functions=[sum(Income#678), sum(POP_2010#554L), count(1)], output=[COMM#545, IncomePerPerson#1362, CrimesPerPerson#1365])\n",
      "         +- Exchange hashpartitioning(COMM#545, 200), ENSURE_REQUIREMENTS, [plan_id=782]\n",
      "            +- HashAggregate(keys=[COMM#545], functions=[partial_sum(Income#678), partial_sum(POP_2010#554L), partial_count(1)], output=[COMM#545, sum#1372, sum#1373L, count#1374L])\n",
      "               +- Project [Income#678, COMM#545, POP_2010#554L]\n",
      "                  +- BroadcastIndexJoin geom#1098: geometry, RightSide, LeftSide, Inner, WITHIN ST_WITHIN(geom#1098, geometry#532)\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1098]\n",
      "                     :  +- Filter ((((isnotnull(LAT#262) AND isnotnull(LON#263)) AND NOT (LAT#262 = 0.0)) AND NOT (LON#263 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :     +- FileScan parquet [LAT#262,LON#263] Batched: true, DataFilters: [isnotnull(LAT#262), isnotnull(LON#263), NOT (LAT#262 = 0.0), NOT (LON#263 = 0.0), isnotnull( **o..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/CrimeData...., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "                     +- SpatialIndex geometry#532: geometry, RTREE, false, false\n",
      "                        +- Project [Income#678, COMM#545, POP_2010#554L, geometry#532]\n",
      "                           +- BroadcastHashJoin [ZipCode#672], [cast(ZCTA10#562 as int)], Inner, BuildRight, false\n",
      "                              :- Project [ZipCode#672, cast(regexp_replace(Income#674, [$,], , 1) as float) AS Income#678]\n",
      "                              :  +- Filter isnotnull(ZipCode#672)\n",
      "                              :     +- FileScan csv [ZipCode#672,Income#674] Batched: false, DataFilters: [isnotnull(ZipCode#672)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/LA_income_..., PartitionFilters: [], PushedFilters: [IsNotNull(ZipCode)], ReadSchema: struct<ZipCode:int,Income:string>\n",
      "                              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[2, string, true] as int) as bigint)),false), [plan_id=774]\n",
      "                                 +- Project [features#529.properties.COMM AS COMM#545, features#529.properties.POP_2010 AS POP_2010#554L, features#529.properties.ZCTA10 AS ZCTA10#562, features#529.geometry AS geometry#532]\n",
      "                                    +- Filter ((isnotnull(features#529.properties.CITY) AND (features#529.properties.CITY = Los Angeles)) AND (isnotnull(features#529.properties.ZCTA10) AND isnotnull(features#529.geometry)))\n",
      "                                       +- Generate explode(features#521), false, [features#529]\n",
      "                                          +- Filter ((size(features#521, true) > 0) AND isnotnull(features#521))\n",
      "                                             +- FileScan geojson [features#521] Batched: false, DataFilters: [(size(features#521, true) > 0), isnotnull(features#521)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/takis/Documents/GitHub/Advanced-DB-2025/Data/2010_Censu..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "+------------------+------------------+--------------------+\n",
      "|              COMM|   IncomePerPerson|     CrimesPerPerson|\n",
      "+------------------+------------------+--------------------+\n",
      "|          Westlake| 47.06267163013844|0.001818971610681...|\n",
      "|     Panorama City| 59.95984645959698|0.001604741028536...|\n",
      "| Little Bangladesh|  77.4462882372383|0.001979373403452...|\n",
      "|     Baldwin Hills| 78.78636074000164|0.002120942479621...|\n",
      "|   Wilshire Center| 81.12746576217563|0.002112811398485...|\n",
      "|   University Park|  84.2864598957781|0.003648016379738...|\n",
      "|         Thai Town| 92.49907318881583|0.001792791417556...|\n",
      "|Wholesale District| 95.40555901521596|0.004404426023303084|\n",
      "|     Vermont Vista| 95.87913381880738|0.003186652379442434|\n",
      "|        South Park| 96.21397332574081|0.003385019532433...|\n",
      "|        Pico-Union|101.48672177250172|0.003285527032422...|\n",
      "|    Toluca Terrace|102.67364930834147|0.002117026109988...|\n",
      "|        West Adams| 104.8323250501383|0.003550872219509559|\n",
      "|          Van Nuys|105.51128525034808|0.002421671095030...|\n",
      "|    Vernon Central|106.84761010812721|0.003680139177439...|\n",
      "|         Koreatown|108.28659873027459| 0.00312340924152636|\n",
      "|           Central|108.98106070862619|0.003616755441572...|\n",
      "|    East Hollywood|112.86302381443797|0.002813818785432924|\n",
      "|       North Hills|114.96038575822709|0.002028475704098...|\n",
      "|   Harvard Heights|116.04382406428836|0.003217299281562349|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('broadcast'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('broadcast'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_merge = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "result_df_merge = result_df_merge.join(\n",
    "    income_per_area_df.hint('merge'),\n",
    "    ST_Within(result_df_merge['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "#result_df_merge.show()\n",
    "\n",
    "result_df_merge.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_hash'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_hush'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "#result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = crimes_df.withColumn('geom', ST_Point(col('LON'), col('LAT')))\n",
    "\n",
    "income_per_area_df = income_df.join(\n",
    "    la_flattened_df.hint('shuffle_replicate_nl'), \n",
    "    col('ZipCode') == la_flattened_df['ZCTA10'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "result_df = result_df.join(\n",
    "    income_per_area_df.hint('shuffle_replicate_nl'),\n",
    "    ST_Within(result_df['geom'], la_flattened_df['geometry']),\n",
    "    how='inner'\n",
    ").groupBy('COMM').agg(\n",
    "    (sum('Income') / sum('POP_2010')).alias('IncomePerPerson'),\n",
    "    (count('*') / sum('POP_2010')).alias('CrimesPerPerson')\n",
    ").orderBy(col('IncomePerPerson').asc())\n",
    "\n",
    "result_df.explain(True)\n",
    "\n",
    "#result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 4 χρησιμοποιώντας το DataFrame ή SQL API. Να εκτελέσετε την υλοποίησή σας εφαρμόζοντας κλιμάκωση στο σύνολο των υπολογιστικών πόρων που θα χρησιμοποιήσετε: Συγκεκριμένα, καλείστε να εκτελέστε την υλοποίησή σας σε 2 executors με τα ακόλουθα configurations:\n",
    "\n",
    "- 1 core/2 GB memory\n",
    "- 2 cores/4GB memory\n",
    "- 4 cores/8GB memory\n",
    "\n",
    "Σχολιάστε τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 core / 2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 1\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4117|\n",
      "|               Other| 1028|\n",
      "|               White|  924|\n",
      "|               Black|  809|\n",
      "|         Other Asian|  289|\n",
      "|             Unknown|  254|\n",
      "|              Korean|  196|\n",
      "|            Filipino|   23|\n",
      "|American Indian/A...|    4|\n",
      "|             Chinese|    4|\n",
      "|    Pacific Islander|    3|\n",
      "|           Guamanian|    3|\n",
      "|            Japanese|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 cores / 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 2\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4117|\n",
      "|               Other| 1028|\n",
      "|               White|  924|\n",
      "|               Black|  809|\n",
      "|         Other Asian|  289|\n",
      "|             Unknown|  254|\n",
      "|              Korean|  196|\n",
      "|            Filipino|   23|\n",
      "|American Indian/A...|    4|\n",
      "|             Chinese|    4|\n",
      "|           Guamanian|    3|\n",
      "|    Pacific Islander|    3|\n",
      "|            Japanese|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 cores / 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark.newSession() \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 Part 3\") \\\n",
    "    .config('spark.executor.instances','2') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = spark.read.csv(fcodes, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_income_df = result_df.orderBy(col('IncomePerPerson').desc()).limit(3) # Using the table in Query 3\n",
    "lowest_income_df = result_df.orderBy('IncomePerPerson').limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_2015_df = q4.withColumn('DATE OCC', to_timestamp('DATE OCC','MM/dd/yyyy hh:mm:ss a'))\n",
    "crimes_2015_df = crimes_2015_df.filter(year(col('DATE OCC')) == 2015)\n",
    "\n",
    "high_income_crimes = crimes_2015_df.join(\n",
    "    highest_income_df,\n",
    "    crimes_2015_df['COMM'] == highest_income_df['COMM']\n",
    ")\n",
    "\n",
    "low_income_crimes = crimes_2015_df.join(\n",
    "    lowest_income_df,\n",
    "    crimes_2015_df['COMM'] == lowest_income_df['COMM']\n",
    ")\n",
    "\n",
    "high_income_race = high_income_crimes.join(\n",
    "    code_df,\n",
    "    high_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")\n",
    "\n",
    "low_income_race = low_income_crimes.join(\n",
    "    code_df,\n",
    "    low_income_crimes['Vict Descent'] == code_df['Vict Descent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|               White|  150|\n",
      "|               Other|   29|\n",
      "|Hispanic/Latin/Me...|   23|\n",
      "|               Black|   16|\n",
      "|         Other Asian|    8|\n",
      "|             Unknown|    5|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_high_df = high_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_high_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Vict Descent Full|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 4117|\n",
      "|               Other| 1028|\n",
      "|               White|  924|\n",
      "|               Black|  809|\n",
      "|         Other Asian|  289|\n",
      "|             Unknown|  254|\n",
      "|              Korean|  196|\n",
      "|            Filipino|   23|\n",
      "|             Chinese|    4|\n",
      "|American Indian/A...|    4|\n",
      "|           Guamanian|    3|\n",
      "|    Pacific Islander|    3|\n",
      "|            Japanese|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_low_df = low_income_race.groupBy('Vict Descent Full').count().alias('Count').orderBy(col('Count').desc())\n",
    "\n",
    "results_low_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
