{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Περιεχόμενα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to be used\n",
    "\n",
    "fcrime = \"../Data/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "fstations = \"../Data/LA_Police_Stations.csv\"\n",
    "fcrime_parq = '../Data/Crime.parquet'\n",
    "fstations_parq = '../Data/Stations.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 10:33:07 WARN Utils: Your hostname, takis-TUF-Gaming-FX505DT-FX505DT resolves to a loopback address: 127.0.1.1; using 192.168.1.217 instead (on interface enp2s0)\n",
      "25/01/17 10:33:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/17 10:33:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF/SQL API\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", DateType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.csv(fcrime, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations table\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    crimes_df.AREA.cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(crimes_df.DateRptd).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table rsults\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 10:33:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date Rptd, AREA , Status\n",
      " Schema: DateRptd, AREA, Status\n",
      "Expected: DateRptd but found: Date Rptd\n",
      "CSV file: file:///home/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data_from_2010_to_2019_20241101.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of DF spark session to completion: 13.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time = time_end - time_start\n",
    "print(f'Time taken since creation of DF spark session to completion: {df_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 10:33:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Crimes Table\n",
    "\n",
    "crimes_schema_sql = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", StringType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"CrmCdDesc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"VictAge\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df = spark.read.format('csv') \\\n",
    "    .options(header='true', dateFormat='MM/dd/yyyy hh:mm:ss a') \\\n",
    "    .schema(crimes_schema_sql) \\\n",
    "    .load(fcrime)\n",
    "\n",
    "crimes_df = crimes_df.withColumn(\"DateRptd\", to_timestamp(\"DateRptd\", \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "                     .withColumn(\"DATEOCC\", to_timestamp(\"DATEOCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations Table\n",
    "\n",
    "stations_schema_sql = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(stations_schema_sql) \\\n",
    "    .load(fstations)\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql = \"\"\"WITH YearlyPrecinctStats AS ( \n",
    "    SELECT \n",
    "        YEAR(c.DateRptd) AS year,\n",
    "        s.DIVISION AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        SUM(CASE WHEN c.Status != 'IC' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS closed_case_rate\n",
    "    FROM crimes c\n",
    "    JOIN stations s\n",
    "        ON CAST(c.AREA as INTEGER) = s.FID\n",
    "    GROUP BY YEAR(c.DateRptd), s.DIVISION\n",
    "    ),\n",
    "    rankedPrecincts AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM YearlyPrecinctStats\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM RankedPrecincts\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY year, ranking;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__ \n",
    "\n",
    "YearlyPrecinctStats\n",
    "- We need to group our data by year and department\n",
    "- Keep a count of all cases and a count of closed cases\n",
    "- Create the rate as a percentage\n",
    "\n",
    "RankedPrecincts\n",
    "- From YearlyPrecinctStats keep: year, precinct and closed_case_rate\n",
    "- We will need to create the ranking based on the closed cases rate of each department\n",
    "- For each year assign a ranking (starting at 1) in descending order\n",
    "\n",
    "Notes:\n",
    "\n",
    "The symbol '#' is not supported in SQL so it was changed to ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 10:33:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date Rptd, AREA , Status\n",
      " Schema: DateRptd, AREA, Status\n",
      "Expected: DateRptd but found: Date Rptd\n",
      "CSV file: file:///home/takis/Documents/GitHub/Advanced-DB-2025/Data/Crime_Data_from_2010_to_2019_20241101.csv\n",
      "25/01/17 10:33:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 8:=======================>                                   (2 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+\n",
      "|year|  precinct| closed_case_rate|ranking|\n",
      "+----+----------+-----------------+-------+\n",
      "|2010| SOUTHEAST|32.94735585531813|      1|\n",
      "|2010|DEVONSHIRE|31.96270619172842|      2|\n",
      "|2010| SOUTHWEST|29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.21216768916155|      1|\n",
      "|2011| SOUTHEAST|32.51177963030083|      2|\n",
      "|2011| SOUTHWEST|28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.41481831052383|      1|\n",
      "|2012| SOUTHEAST|32.94641810294290|      2|\n",
      "|2012| SOUTHWEST|29.81513327601032|      3|\n",
      "|2013|DEVONSHIRE|33.52812271731191|      1|\n",
      "|2013| SOUTHEAST|32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.16422459266206|      3|\n",
      "|2014|HOLLENBECK|31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.31198995605775|      2|\n",
      "|2014|  FOOTHILL|31.16279069767442|      3|\n",
      "|2015|HOLLENBECK|32.64134698172773|      1|\n",
      "|2015|  WILSHIRE|30.27597402597403|      2|\n",
      "|2015|  FOOTHILL|30.17946067838016|      3|\n",
      "|2016|HOLLENBECK|31.88075572011773|      1|\n",
      "|2016|  WILSHIRE|31.54798761609907|      2|\n",
      "+----+----------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken since creation of spark session to query completion: 6.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query2_sql).show()\n",
    "time_end = time.time()\n",
    "sql_time = time_end - time_start\n",
    "print(f\"Time taken since creation of spark session to query completion: {sql_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for DataFrame API: 13.68\n",
      "Time taken for SQL API: 6.37\n"
     ]
    }
   ],
   "source": [
    "print(f'''Time taken for DataFrame API: {df_time:.2f}\n",
    "Time taken for SQL API: {sql_time:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ο χρόνος εκτέλεσης του DataFrame API είναι περίπου διπλάσιος από του SQL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crimes_df = spark.read.csv(fcrime,header=True, inferSchema=True)\n",
    "crimes_df.write.mode('overwrite').parquet(fcrime_parq)\n",
    "\n",
    "stations_df = spark.read.csv(fstations,header=True, inferSchema=True)\n",
    "stations_df.write.mode('overwrite').parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιήσουμε το DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.parquet(fcrime_parq)\n",
    "stations_df = spark.read.parquet(fstations_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = crimes_df.withColumn('Date Rptd', to_timestamp('Date Rptd', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table yearly_precincts\n",
    "\n",
    "yearly_precincts_df = crimes_df.join(\n",
    "    stations_df,\n",
    "    col('AREA ').cast(\"int\") == stations_df.FID\n",
    ").groupBy(\n",
    "    year(col('Date Rptd')).alias(\"year\"),\n",
    "    stations_df.DIVISION.alias(\"precinct\")\n",
    ").agg(\n",
    "    sum(when(col('Status') != \"IC\", 1).otherwise(0)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    (\n",
    "        sum(when(col('Status') != \"IC\", 1).otherwise(0)) * 100.0 / count(\"*\")\n",
    "    ).alias(\"closed_case_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table ranked precincts\n",
    "\n",
    "windowSpec = Window.partitionBy('year').orderBy(col('closed_case_rate').desc())\n",
    "\n",
    "ranked_precincts_df = yearly_precincts_df.withColumn(\n",
    "    'ranking', row_number().over(windowSpec)\n",
    ").select(\n",
    "    col('year'),\n",
    "    col('precinct'),\n",
    "    col('closed_case_rate'),\n",
    "    col('ranking')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table results\n",
    "\n",
    "results_df = ranked_precincts_df.filter(\n",
    "    col('ranking') <= 3\n",
    ").select(\n",
    "    'year',\n",
    "    'precinct',\n",
    "    'closed_case_rate',\n",
    "    'ranking'\n",
    ").orderBy('year','ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+-------+\n",
      "|year|  precinct|  closed_case_rate|ranking|\n",
      "+----+----------+------------------+-------+\n",
      "|2010| SOUTHEAST|32.947355855318136|      1|\n",
      "|2010|DEVONSHIRE|31.962706191728426|      2|\n",
      "|2010| SOUTHWEST| 29.63203463203463|      3|\n",
      "|2011|DEVONSHIRE|35.212167689161554|      1|\n",
      "|2011| SOUTHEAST|32.511779630300836|      2|\n",
      "|2011| SOUTHWEST| 28.65220520201501|      3|\n",
      "|2012|DEVONSHIRE|34.414818310523835|      1|\n",
      "|2012| SOUTHEAST|  32.9464181029429|      2|\n",
      "|2012| SOUTHWEST|29.815133276010318|      3|\n",
      "|2013|DEVONSHIRE| 33.52812271731191|      1|\n",
      "|2013| SOUTHEAST| 32.08287360549222|      2|\n",
      "|2013| SOUTHWEST|29.164224592662055|      3|\n",
      "|2014|HOLLENBECK| 31.80567315834039|      1|\n",
      "|2014|  WILSHIRE|31.311989956057754|      2|\n",
      "|2014|  FOOTHILL|31.162790697674417|      3|\n",
      "|2015|HOLLENBECK|32.641346981727736|      1|\n",
      "|2015|  WILSHIRE|30.275974025974026|      2|\n",
      "|2015|  FOOTHILL|30.179460678380156|      3|\n",
      "|2016|HOLLENBECK|31.880755720117726|      1|\n",
      "|2016|  WILSHIRE| 31.54798761609907|      2|\n",
      "+----+----------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for DF with parquet to completion: 3.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Final results:\n",
    "\n",
    "results_df.show()\n",
    "time_end = time.time()\n",
    "df_time_parq = time_end - time_start\n",
    "print(f'Time taken for DF with parquet to completion: {df_time_parq:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας τα parquet δεδομένα, φαίνεται μια αρκετά μεγάλη βελτίωση στην ταχύτητα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
